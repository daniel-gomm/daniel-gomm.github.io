<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Practical Guide to the OpenAI Batch API with Python and openbatch | Daniel Gomm </title> <meta name="author" content="Daniel Gomm"> <meta name="description" content="This post introduces `openbatch`, a Python library designed to make the powerful but often cumbersome OpenAI Batch API as convenient and easy to use as standard sequential calls."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://daniel-gomm.github.io/blog/2025/openbatch/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Daniel</span> Gomm </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Practical Guide to the OpenAI Batch API with Python and openbatch</h1> <p class="post-meta"> Created in September 29, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/code"> <i class="fa-solid fa-hashtag fa-sm"></i> code</a>   <a href="/blog/tag/guide"> <i class="fa-solid fa-hashtag fa-sm"></i> guide</a>   <a href="/blog/tag/openai"> <i class="fa-solid fa-hashtag fa-sm"></i> openai</a>   ·   <a href="/blog/category/project"> <i class="fa-solid fa-tag fa-sm"></i> project</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>TL;DR The OpenAI Batch API offers a <strong>50% cost reduction</strong> and significantly higher throughput for large-scale tasks, but its file-based workflow is cumbersome, especially for structured outputs. The <a href="https://daniel-gomm.github.io/openbatch"><code class="language-plaintext highlighter-rouge">openbatch</code></a> Python library eliminates this friction by providing a convenient, <strong>drop-in replacement for the standard OpenAI client</strong>. It simplifies batch file creation, streamlines structured data handling using Pydantic, and offers powerful templating, making the cost and speed benefits of batch processing easily accessible without sacrificing developer convenience.</p> </blockquote> <p>For researchers and developers working with large datasets, the <a href="https://platform.openai.com/docs/guides/batch/batch-api" rel="external nofollow noopener" target="_blank">OpenAI Batch API</a> offers significant advantages in cost and speed. However, its asynchronous, file-based workflow can feel cumbersome compared to the simplicity of direct API calls. This guide explores the trade-offs and introduces <a href="https://daniel-gomm.github.io/openbatch">openbatch</a>, a Python package designed to make the Batch API as convenient to use as the standard sequential API.</p> <h2 id="batch-processing-is-cheaper-and-often-faster">Batch Processing is Cheaper and often Faster</h2> <p>The primary motivation for using the Batch API is efficiency. It provides two key benefits:</p> <ol> <li> <strong>50% Cost Reduction</strong>: Batch API calls are priced at <strong>half the cost</strong> of the standard API. For large-scale data analysis, classification, or generation tasks, this immediately doubles your budget’s effectiveness.</li> <li> <strong>High Throughput</strong>: While batch jobs have a 24-hour completion window, they often finish much faster than an equivalent number of sequential calls. In one experiment, a task that would have taken over <strong>10 hours of sequential API calls</strong> was <strong>completed in under 1 hour</strong> using a single batch job.</li> </ol> <p>The trade-off for this (cost-)efficiency has traditionally been convenience. Instead of a simple request-response cycle, the batch workflow involves manually preparing a JSONL file, uploading it, starting the job, and then retrieving the results from a separate file. This is especially cumbersome and challenging when you need to generate prompts dynamically or make use of <a href="https://platform.openai.com/docs/guides/structured-outputs" rel="external nofollow noopener" target="_blank">structured outputs</a>.</p> <h2 id="convenient-batch-processing-with-openbatch">Convenient Batch Processing with <code class="language-plaintext highlighter-rouge">openbatch</code> </h2> <p><code class="language-plaintext highlighter-rouge">openbatch</code> is a lightweight Python library that simplifies the creation of the batch input file by providing a developer experience that mirrors the official <code class="language-plaintext highlighter-rouge">openai</code> Python client. It’s designed to be a near drop-in replacement that can integrate into existing workflows, allowing you to switch between sequential and batch processing with minimal code changes.</p> <p>The library’s core features directly address the common pain points of the batch workflow:</p> <ul> <li> <strong>Familiar API</strong>: The <code class="language-plaintext highlighter-rouge">BatchCollector</code> class mimics the structure of <code class="language-plaintext highlighter-rouge">openai.OpenAI</code>, so you can write <code class="language-plaintext highlighter-rouge">collector.responses.create(...)</code> instead of <code class="language-plaintext highlighter-rouge">client.responses.create(...)</code>.</li> <li> <strong>Structured Outputs with Pydantic</strong>: Reliably getting JSON output from LLMs can be tricky. <code class="language-plaintext highlighter-rouge">openbatch</code> allows you to pass a Pydantic model directly to a <code class="language-plaintext highlighter-rouge">parse()</code> method (e.g., <code class="language-plaintext highlighter-rouge">collector.responses.parse(text_format=MyModel)</code>), which automatically handles the complex JSON schema generation needed to enforce the output structure.</li> <li> <strong>Powerful Templating</strong>: With the <code class="language-plaintext highlighter-rouge">BatchJobManager</code>, you can define a <code class="language-plaintext highlighter-rouge">PromptTemplate</code> with placeholders and programmatically generate thousands or millions of requests from a list of data, which is ideal for large-scale, repetitive tasks.</li> <li> <strong>Full API Coverage</strong>: It supports all endpoints available in the Batch API: <code class="language-plaintext highlighter-rouge">/v1/responses</code>, <code class="language-plaintext highlighter-rouge">/v1/chat/completions</code>, and <code class="language-plaintext highlighter-rouge">/v1/embeddings</code>.</li> </ul> <p>Have a look at the full documentation at <a href="https://daniel-gomm.github.io/openbatch">https://daniel-gomm.github.io/openbatch</a> for more details and how-to guides.</p> <h3 id="installation">Installation</h3> <p>You can install <code class="language-plaintext highlighter-rouge">openbatch</code> via pip:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>openbatch
</code></pre></div></div> <h3 id="a-practical-workflow-sentiment-analysis-at-scale">A Practical Workflow: Sentiment Analysis at Scale</h3> <p>To understand the value of <code class="language-plaintext highlighter-rouge">openbatch</code>, let’s walk through a common, real-world task: performing sentiment analysis on a large dataset of customer reviews. Imagine you have a file, <code class="language-plaintext highlighter-rouge">customer_reviews.csv</code>, containing thousands of reviews that you need to classify.</p> <p>The goal is to get a structured output for each review, classifying it as ‘Positive’, ‘Neutral’, or ‘Negative’ with a confidence score. We’ll start with a standard sequential approach and then see how <code class="language-plaintext highlighter-rouge">openbatch</code> dramatically simplifies the process for batching.</p> <p>First, let’s define the Pydantic model that will enforce our desired output structure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Literal</span>

<span class="k">class</span> <span class="nc">SentimentAnalysisModel</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">sentiment</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="sh">"</span><span class="s">Positive</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Neutral</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Negative</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">confidence</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span>
        <span class="n">ge</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
        <span class="n">le</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">Confidence score for the sentiment classification.</span><span class="sh">"</span>
    <span class="p">)</span>
</code></pre></div></div> <p>Now, let’s assume our <code class="language-plaintext highlighter-rouge">customer_reviews.csv</code> looks like this:</p> <table> <thead> <tr> <th>review_id</th> <th>review_text</th> </tr> </thead> <tbody> <tr> <td>a-123</td> <td>The product is absolutely fantastic!</td> </tr> <tr> <td>b-456</td> <td>It broke after just one week of use.</td> </tr> <tr> <td>c-789</td> <td>The packaging was okay.</td> </tr> <tr> <td>…</td> <td><em>(thousands more rows)</em></td> </tr> </tbody> </table> <h4 id="approach-1-the-standard-sequential-loop">Approach 1: The Standard Sequential Loop</h4> <p>A typical approach would be to loop through the dataset and make an API call for each review. This is easy to write but highly inefficient for large datasets.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">csv</span>
<span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">instructions</span> <span class="o">=</span> <span class="sh">"</span><span class="s">You are an expert annotator. Judge the sentiment of the user-provided comment. Use the categories </span><span class="sh">'</span><span class="s">Positive</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">Neutral</span><span class="sh">'</span><span class="s">, and </span><span class="sh">'</span><span class="s">Negative</span><span class="sh">'</span><span class="s">.</span><span class="sh">"</span>

<span class="c1"># This loop is slow and expensive at scale
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Starting sequential processing...</span><span class="sh">"</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">customer_reviews.csv</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="nc">DictReader</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">responses</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">instructions</span><span class="o">=</span><span class="n">instructions</span><span class="p">,</span>
            <span class="nb">input</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_text</span><span class="sh">"</span><span class="p">]}],</span>
            <span class="n">text_format</span><span class="o">=</span><span class="n">SentimentAnalysisModel</span>
        <span class="p">)</span>
        <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">review_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_id</span><span class="sh">"</span><span class="p">],</span> <span class="sh">"</span><span class="s">analysis</span><span class="sh">"</span><span class="p">:</span> <span class="n">response</span><span class="p">})</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Processed review </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">review_id</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">✅ Sequential processing complete.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This method is simple but has major drawbacks: it’s <strong>slow</strong>, processing one review at a time, and <strong>expensive</strong>, as it uses standard API pricing.</p> <h4 id="approach-2-the-batchcollector-drop-in-replacement">Approach 2: The <code class="language-plaintext highlighter-rouge">BatchCollector</code> Drop-in Replacement</h4> <p>Here’s where <code class="language-plaintext highlighter-rouge">openbatch</code> comes in. You can switch to batch processing with minimal changes. The <code class="language-plaintext highlighter-rouge">BatchCollector</code> API is designed to be a <strong>drop-in replacement</strong> for the <code class="language-plaintext highlighter-rouge">OpenAI</code> client within your existing loop.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">csv</span>
<span class="kn">from</span> <span class="n">openbatch</span> <span class="kn">import</span> <span class="n">BatchCollector</span>

<span class="n">batch_file_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sentiment_batch.jsonl</span><span class="sh">"</span>
<span class="n">collector</span> <span class="o">=</span> <span class="nc">BatchCollector</span><span class="p">(</span><span class="n">batch_file_path</span><span class="o">=</span><span class="n">batch_file_path</span><span class="p">)</span> <span class="c1"># Change 1: Instantiate collector
</span><span class="n">instructions</span> <span class="o">=</span> <span class="sh">"</span><span class="s">You are an expert annotator. Judge the sentiment of the user-provided comment. Use the categories </span><span class="sh">'</span><span class="s">Positive</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">Neutral</span><span class="sh">'</span><span class="s">, and </span><span class="sh">'</span><span class="s">Negative</span><span class="sh">'</span><span class="s">.</span><span class="sh">"</span>

<span class="c1"># The same loop, now preparing a batch file instead of making live calls
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Preparing batch file with BatchCollector...</span><span class="sh">"</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">customer_reviews.csv</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="nc">DictReader</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
        <span class="c1"># Change 2: Call the collector instead of the client
</span>        <span class="n">collector</span><span class="p">.</span><span class="n">responses</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span>
            <span class="n">custom_id</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_id</span><span class="sh">"</span><span class="p">],</span> <span class="c1"># Add a custom ID for tracking
</span>            <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">instructions</span><span class="o">=</span><span class="n">instructions</span><span class="p">,</span>
            <span class="nb">input</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_text</span><span class="sh">"</span><span class="p">]}],</span>
            <span class="n">text_format</span><span class="o">=</span><span class="n">SentimentAnalysisModel</span>
        <span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">✅ Batch file </span><span class="sh">'</span><span class="si">{</span><span class="n">batch_file_path</span><span class="si">}</span><span class="sh">'</span><span class="s"> prepared.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>With just two lines changed, your code now prepares a cost-effective batch job instead of making slow, expensive API calls. You’ve decoupled file preparation from execution.</p> <h4 id="approach-3-the-simplified-batchjobmanager">Approach 3: The Simplified <code class="language-plaintext highlighter-rouge">BatchJobManager</code> </h4> <p>For templated, large-scale tasks like this, the <code class="language-plaintext highlighter-rouge">BatchJobManager</code> is even more efficient. It eliminates the need for an explicit Python loop entirely, letting you define the task and the data separately.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">csv</span>
<span class="kn">from</span> <span class="n">openbatch</span> <span class="kn">import</span> <span class="n">BatchJobManager</span><span class="p">,</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">Message</span><span class="p">,</span> <span class="n">ResponsesRequest</span><span class="p">,</span> <span class="n">PromptTemplateInputInstance</span>

<span class="c1"># 1. Define the prompt template
</span><span class="n">template</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[</span>
    <span class="nc">Message</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="sh">"</span><span class="s">You are an expert annotator. Judge the sentiment of the user-provided comment. Use the categories </span><span class="sh">'</span><span class="s">Positive</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">Neutral</span><span class="sh">'</span><span class="s">, and </span><span class="sh">'</span><span class="s">Negative</span><span class="sh">'</span><span class="s">.</span><span class="sh">"</span><span class="p">),</span>
    <span class="nc">Message</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="sh">"</span><span class="s">{review_text}</span><span class="sh">"</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># 2. Define the common request configuration
</span><span class="n">common_config</span> <span class="o">=</span> <span class="nc">ResponsesRequest</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Enforce the structured output for all requests in this batch
</span><span class="n">common_config</span><span class="p">.</span><span class="nf">set_output_structure</span><span class="p">(</span><span class="n">SentimentAnalysisModel</span><span class="p">)</span>

<span class="c1"># 3. Load the data and create input instances (no loop needed for API calls)
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">customer_reviews.csv</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="nc">DictReader</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">input_instances</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nc">PromptTemplateInputInstance</span><span class="p">(</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_id</span><span class="sh">"</span><span class="p">],</span>
            <span class="n">prompt_value_mapping</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">review_text</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_text</span><span class="sh">"</span><span class="p">]}</span>
        <span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span>
    <span class="p">]</span>

<span class="c1"># 4. Generate the entire batch file in one go
</span><span class="n">batch_file_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sentiment_batch_optimized.jsonl</span><span class="sh">"</span>
<span class="n">manager</span> <span class="o">=</span> <span class="nc">BatchJobManager</span><span class="p">()</span>
<span class="n">manager</span><span class="p">.</span><span class="nf">add_templated_instances</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">template</span><span class="p">,</span>
    <span class="n">common_request</span><span class="o">=</span><span class="n">common_config</span><span class="p">,</span>
    <span class="n">input_instances</span><span class="o">=</span><span class="n">input_instances</span><span class="p">,</span>
    <span class="n">save_file_path</span><span class="o">=</span><span class="n">batch_file_path</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Batch file </span><span class="sh">'</span><span class="si">{</span><span class="n">batch_file_path</span><span class="si">}</span><span class="sh">'</span><span class="s"> generated with BatchJobManager.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This approach is the cleanest and most declarative, perfectly suited for large-scale, programmatic batch job creation.</p> <h4 id="creating-the-batch-job">Creating the Batch Job</h4> <p>Once the batch-job-file is created, the batch-job can be started. This is possible through the <a href="https://platform.openai.com/batches" rel="external nofollow noopener" target="_blank">Batches Web UI</a> or programatically via the standard <code class="language-plaintext highlighter-rouge">openai</code> client:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">()</span>

<span class="c1"># Upload the file to OpenAI
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Uploading </span><span class="sh">'</span><span class="si">{</span><span class="n">batch_file_path</span><span class="si">}</span><span class="sh">'</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">batch_input_file</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">files</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="nb">file</span><span class="o">=</span><span class="nf">open</span><span class="p">(</span><span class="n">batch_file_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">purpose</span><span class="o">=</span><span class="sh">"</span><span class="s">batch</span><span class="sh">"</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">File uploaded. File ID: </span><span class="si">{</span><span class="n">batch_input_file</span><span class="p">.</span><span class="nb">id</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Create the batch job, specifying the correct endpoint
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Creating batch job...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">batch_job</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">batches</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">input_file_id</span><span class="o">=</span><span class="n">batch_input_file</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="sh">"</span><span class="s">/v1/responses</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Must match the API used in your requests
</span>    <span class="n">completion_window</span><span class="o">=</span><span class="sh">"</span><span class="s">24h</span><span class="sh">"</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Batch job created. Job ID: </span><span class="si">{</span><span class="n">batch_job</span><span class="p">.</span><span class="nb">id</span><span class="si">}</span><span class="s">, Status: </span><span class="si">{</span><span class="n">batch_job</span><span class="p">.</span><span class="n">status</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h4 id="monitor-and-download-the-results">Monitor and Download the Results</h4> <p>After some time, you can check the job’s status. Once it’s complete, you can download the output file containing the responses. This is again possible through the <a href="https://platform.openai.com/batches" rel="external nofollow noopener" target="_blank">Batches Web UI</a> or programatically:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In a real application, you could poll this endpoint periodically
</span><span class="n">completed_job</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">batches</span><span class="p">.</span><span class="nf">retrieve</span><span class="p">(</span><span class="n">batch_job</span><span class="p">.</span><span class="nb">id</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Current job status: </span><span class="si">{</span><span class="n">completed_job</span><span class="p">.</span><span class="n">status</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Once the status is 'completed'
</span><span class="k">if</span> <span class="n">completed_job</span><span class="p">.</span><span class="n">status</span> <span class="o">==</span> <span class="sh">'</span><span class="s">completed</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">output_file_id</span> <span class="o">=</span> <span class="n">completed_job</span><span class="p">.</span><span class="n">output_file_id</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Job finished! Output file ID: </span><span class="si">{</span><span class="n">output_file_id</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Download the results file
</span>    <span class="n">results_content</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">files</span><span class="p">.</span><span class="nf">content</span><span class="p">(</span><span class="n">file_id</span><span class="o">=</span><span class="n">output_file_id</span><span class="p">)</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">batch_results.jsonl</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">results_content</span><span class="p">.</span><span class="nf">read</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">✅ Results saved to </span><span class="sh">'</span><span class="s">batch_results.jsonl</span><span class="sh">'</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">completed_job</span><span class="p">.</span><span class="n">status</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">failed</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">expired</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">cancelled</span><span class="sh">'</span><span class="p">]:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">❌ Job did not complete. Status: </span><span class="si">{</span><span class="n">completed_job</span><span class="p">.</span><span class="n">status</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>The downloaded <code class="language-plaintext highlighter-rouge">batch_results.jsonl</code> file will contain the output for each of the input requests, which can then be parsed for the application. Thanks to the structured output feature, the JSON responses for the requests will be clean, valid, and ready to be loaded directly into the Pydantic models.</p> <h2 id="conclusion">Conclusion</h2> <p>The OpenAI Batch API is a powerful tool for processing large datasets efficiently and cost-effectively. While its asynchronous nature can introduce complexity, libraries like <a href="https://daniel-gomm.github.io/openbatch"><code class="language-plaintext highlighter-rouge">openbatch</code></a> abstract away the tedious work of file preparation. By providing a familiar, Pydantic-powered interface, it makes the benefits of batch processing accessible without sacrificing developer convenience, allowing researchers and engineers to focus on their results, not their boilerplate.</p> <p>For more detail on <code class="language-plaintext highlighter-rouge">openbatch</code>, including installation instructions and comprehensive documentation, visit the <a href="https://daniel-gomm.github.io/openbatch">official documentation site</a>.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Table-Serialization-Kitchen/">Table Serialization Kitchen - A Recipe for Better LLM Performance on Tabular Data</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Daniel Gomm. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme and the <a href="https://www.nordtheme.com" target="_blank" rel="external nofollow noopener">Nord colortheme</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: September 29, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my projects. [Work In Progress]",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Overview of repositories for interesting projects that I&#39;ve worked on.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"Feel free to explore my professional and academic journey.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-a-practical-guide-to-the-openai-batch-api-with-python-and-openbatch",title:"A Practical Guide to the OpenAI Batch API with Python and openbatch",description:"This post introduces `openbatch`, a Python library designed to make the powerful but often cumbersome OpenAI Batch API as convenient and easy to use as standard sequential calls.",section:"Posts",handler:()=>{window.location.href="/blog/2025/openbatch/"}},{id:"post-table-serialization-kitchen-a-recipe-for-better-llm-performance-on-tabular-data",title:"Table Serialization Kitchen - A Recipe for Better LLM Performance on Tabular Data...",description:"This blog post explores table serializations and their impact on downstream tasks using the Table Serialization Kitchen.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Table-Serialization-Kitchen/"}},{id:"news-started-scientific-traineeship-at-the-joint-research-center-of-the-european-commission",title:"Started scientific traineeship at the Joint Research Center of the European Commission",description:"",section:"News",handler:()=>{window.location.href="/news/2024_04_01_joined_ecat/"}},{id:"news-joined-centrum-wiskunde-amp-informatica-as-phd-student",title:"Joined Centrum Wiskunde &amp; Informatica as PhD student",description:"",section:"News",handler:()=>{window.location.href="/news/2024_10_01_joined_cwi/"}},{id:"news-started-the-amsterdam-lunch-on-table-alot-reading-group",title:"Started the Amsterdam Lunch on Table (ALOT) reading group",description:"",section:"News",handler:()=>{window.location.href="/news/2025_04_01_established_reading_group/"}},{id:"projects-pytei",title:"PyTEI",description:"A lightweight python client package for Huggingface Text Embedding Inference",section:"Projects",handler:()=>{window.location.href="/projects/PyTEI/"}},{id:"projects-table-serializer-kitchen",title:"Table Serializer Kitchen",description:"A flexible, extensible, and easy-to-use package for serializing tabular data for the use with LLMs.",section:"Projects",handler:()=>{window.location.href="/projects/table-serializer-kitchen/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%64%61%6E%69%65%6C.%67%6F%6D%6D[%61%74]%63%77%69.%6E%6C","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0007-5489-992X","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=JX0iKVMAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/daniel-gomm","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/daniel-gomm","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/https://bsky.app/profile/daniel-gomm.bsky.social# your bluesky URL","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>