---
---

@misc{qu2024greedycodycounterfactualexplainers,
      title={GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs},
      author={Zhan Qu and Daniel Gomm and Michael Faerber},
      year={2024},
      eprint={2403.16846},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.16846},
      abstract={Temporal Graph Neural Networks (TGNNs), crucial for modeling dynamic graphs with time-varying interactions, face a significant challenge in explainability due to their complex model structure. Counterfactual explanations, crucial for understanding model decisions, examine how input graph changes affect outcomes. This paper introduces two novel counterfactual explanation methods for TGNNs: GreeDy (Greedy Explainer for Dynamic Graphs) and CoDy (Counterfactual Explainer for Dynamic Graphs). They treat explanations as a search problem, seeking input graph alterations that alter model predictions. GreeDy uses a simple, greedy approach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm. Experiments show both methods effectively generate clear explanations. Notably, CoDy outperforms GreeDy and existing factual methods, with up to 59\% higher success rate in finding significant counterfactual inputs. This highlights CoDy's potential in clarifying TGNN decision-making, increasing their transparency and trustworthiness in practice.},
      arxiv={2403.16846},
      pdf={https://arxiv.org/pdf/2403.16846},
      bibtex_show={true},
      selected={true}
}

@inproceedings{
gomm2025metadata,
title={Metadata Matters in Dense Table Retrieval},
author={Daniel Gomm and Madelon Hulsebos},
booktitle={ELLIS workshop on Representation Learning and Generative Models for Structured Data},
abstract={Recent advances in Large Language Models have enabled powerful systems that perform tasks by reasoning over tabular data. While these systems typically assume relevant data is provided with a query, real-world use cases are mostly open-domain, meaning they receive a query without context regarding the underlying tables. Retrieving relevant tables is typically done over dense embeddings of serialized tables. Yet, there is a limited understanding of the effectiveness of different inputs and serialization methods for using such off-the-shelf text-embedding models for table retrieval. In this work, we show that different serialization strategies result in significant variations in retrieval performance. Additionally, we surface shortcomings in commonly used benchmarks applied in open-domain settings, motivating further study and refinement.},
year={2025},
url={https://openreview.net/forum?id=rELWIvq2Qy},
pdf={https://openreview.net/pdf?id=rELWIvq2Qy},
bibtex_show={true},
selected={true},
blog={../blog/2025/Table-Serialization-Kitchen}
}
