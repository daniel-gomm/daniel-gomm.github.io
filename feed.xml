<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://daniel-gomm.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://daniel-gomm.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-24T10:37:25+00:00</updated><id>https://daniel-gomm.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Daniel Gomm. </subtitle><entry><title type="html">Have your Queries Already Seen the Data? Data-Privilege in Tabular Benchmarks</title><link href="https://daniel-gomm.github.io/blog/2025/Have-you-Queries-Already-Seen-the-Data/" rel="alternate" type="text/html" title="Have your Queries Already Seen the Data? Data-Privilege in Tabular Benchmarks"/><published>2025-11-14T00:00:00+00:00</published><updated>2025-11-14T00:00:00+00:00</updated><id>https://daniel-gomm.github.io/blog/2025/Have%20you%20Queries%20Already%20Seen%20the%20Data</id><content type="html" xml:base="https://daniel-gomm.github.io/blog/2025/Have-you-Queries-Already-Seen-the-Data/"><![CDATA[<blockquote> <p><strong>Note:</strong> This blog post is a deep-dive into a critical issue surfaced in our recent paper <em>“<a href="https://arxiv.org/abs/2511.04584">Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis</a>“</em> accepted to the <a href="https://sites.google.com/view/eurips25-ai-td/home">AI for Tabular Data Workshop at EurIPS 2025</a>.</p> </blockquote> <blockquote> <p><strong>TL;DR:</strong> Many popular benchmarks for evaluating natural language interfaces to tabular data contain “data-privileged” queries—questions that reference specific database structures, internal codes, or data containers that real users wouldn’t know about in open-domain settings. Our analysis of 15 datasets reveals that up to 70% of queries in complex analysis benchmarks and 26-27% in widely-used text-to-SQL datasets contain such privileged information, fundamentally undermining evaluations by providing unrealistic shortcuts. To properly test open-domain capabilities, we need to either carefully adapt existing datasets to remove privileged information while maintaining realistic data scope specification, or shift to query-first construction methodologies that mirror how real users formulate information needs.</p> </blockquote> <h2 id="when-your-test-data-knows-too-much">When Your Test Data Knows Too Much</h2> <p>Natural language interfaces to databases have come a long way. What started as systems that let users query a single, known database with questions like “How many employees work in the sales department?” are evolving into something far more ambitious: open-domain systems that identify relevant tabular data from vast, unknown corpora before answering queries. This shift is happening across the board—from text-to-SQL generation<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> to question answering<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">3</a></sup> <sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">4</a></sup> <sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">5</a></sup> to full-scale data analysis<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup> <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">7</a></sup>.</p> <p>But there’s a problem hiding in plain sight. As researchers start evaluating open-domain systems they plainly adapt existing datasets from closed-domain settings (where users know exactly which database they’re querying) or device new specific datasets. While users formulate queries over an unknown corpus of tabular data, going from query to data, these datasets are constructed the other way around, starting out with (a) specific table(s) and deriving queries from this. If not explicitly addressed, such dataset construction methods inadvertently introduce an unrealistic advantage into the data: many queries in these benchmarks contain privileged information that real users would never have access to.</p> <p>Consider this query from the HiTab dataset: <em>“What was the number of the ashrs of polish refugees?”</em><sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">8</a></sup> The term “ ashrs” isn’t a word you’d find in a dictionary, it’s a column header copied directly from a specific table. Or this one from DA-Eval: <em>“Check if the RHO_OLD column follows a normal distribution.”</em><sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">9</a></sup> A real user asking questions about data they haven’t seen wouldn’t know that a column called “RHO_OLD” exists, let alone reference it by its exact database identifier.</p> <p>These aren’t isolated examples. In our analysis of 15 popular datasets spanning question answering, text-to-SQL, and data analysis tasks, we found that some benchmarks have up to 70% of their queries containing such privileged information. This matters because when we evaluate systems on these queries, we’re not testing their ability to work in realistic open-domain settings, instead we’re testing them on a fundamentally easier task where users magically know the underlying data structure, directly tying a query to specific data.</p> <h2 id="what-is-data-privilege-and-why-should-we-care">What Is Data-Privilege, and Why Should We Care?</h2> <p>At its core, a data-privileged query is one that betrays knowledge the user shouldn’t have. In a true open-domain setting, users approach the system with information needs grounded in their understanding of the world and the system they are interacting with <sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">10</a></sup>, not in knowledge of how some specific dataset happens to be structured. When a user asks about “quarterly revenue trends for technology companies,” they’re expressing a natural information need. When they ask about “the avg_revenue_q1 column for rows where industry_code=’TECH’,” they’re revealing that they’ve already seen the data.</p> <p>This distinction matters profoundly for evaluation. <em>Data-privileged queries</em> provide an unrealistic signal that makes the task appear simpler than it actually is. They create a shortcut: instead of having to understand the user’s conceptual query and map it to whatever data structures might be relevant in a massive corpus, the system can often pattern-match directly to the referenced structural elements. This fundamentally undermines what we claim to be testing in open-domain scenarios.</p> <p>We identify three distinct manifestations of data-privilege in queries:</p> <h4 id="1-structural-references---when-queries-speak-database">1. Structural References - When Queries Speak Database</h4> <p>Structural references occur when queries use terminology that sounds more like database schema than natural language—phrases that feel “copied” rather than “composed.” The most obvious cases involve programming conventions: queries asking about “SalePrice” (camelCase) or “gdpPercap_1982” (underscores and suffixes) are clearly referencing specific field names. But the pattern can be subtler: asking for “Aaron Doran’s potential score” rather than his “skill level” or “rating” suggests familiarity with how a sports database labels its columns<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">11</a></sup>.</p> <p>Database-specific concepts provide another giveaway. Queries that ask for “the record with index 5” or “the primary key” use language from data management, not everyday information seeking. These concepts are highly indicative of directly interacting with a database instead of leaving this task to the system. Consider this query from TableBench: <em>“What is the correlation between a country’s ‘carbon dioxide emissions per year (tons per person)’ and its ‘average emission per km² of land’?”</em><sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote" rel="footnote">12</a></sup>. Those precise metric definitions in quotes strongly suggest knowledge of exactly how these measurements are labeled in a specific dataset.</p> <h4 id="2-value-references---knowing-the-database-contents">2. Value References - Knowing the Database Contents</h4> <p>Value references reveal knowledge of what specific data values exist in underlying tables. The clearest cases involve internal identifiers, i.e., codes or keys that exist purely within a dataset’s organizing logic. Consider this from MMQA: <em>“Which clubs located in ‘AKW’ have members holding either ‘President’ or ‘Vice-President’ positions?”</em><sup id="fnref:18" role="doc-noteref"><a href="#fn:18" class="footnote" rel="footnote">13</a></sup> Unless “AKW” is publicly known, this three-letter code suggests the user has looked at the table. Similarly, queries about <em>“authors who publish books in both ‘MM’ and ‘LT’ series”</em><sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">14</a></sup> or <em>“clients whose complaint type is ‘TT’“</em><sup id="fnref:5:1" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">11</a></sup> use cryptic codes that point to internal categorization schemes.</p> <p>Not all value references indicate leakage of privileged information. Publicly knowable named entities like people, places, organizations, or dates don’t necessarily indicate data-privilege. Asking about “<a href="https://en.wikipedia.org/wiki/Janja_Garnbret">Janja Garnbret</a>” or “the 2024 Olympics” uses world knowledge, not dataset-specific knowledge. The distinction lies in whether the specificity comes from general knowledge or from having seen the particular data.</p> <h4 id="3-container-references---breaking-the-fourth-wall">3. Container References - Breaking the Fourth Wall</h4> <p>Container references explicitly acknowledge working with a data artifact, breaking the illusion of asking about the world. Phrases like “in the dataset,” “according to the table,” or “using the provided spreadsheet” directly reference the data container. Consider the query <em>“Load the data into the SQLite database”</em> from DA-Code <sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">15</a></sup>, this isn’t maintaining any fiction of open-domain interaction. Even subtle conceptual references like “using the provided dataset, find the top five most frequent qualifications” assume a bounded artifact that has been “provided” rather than discovered.</p> <h2 id="data-privilege-across-15-popular-benchmarks">Data-Privilege Across 15 Popular Benchmarks</h2> <p>To get a grasp on the prevalence of data-privileged queries, we systematically analyzed 15 datasets commonly used to evaluate natural language interfaces to tabular data, spanning single-table question answering, multi-table reasoning, text-to-SQL, and data analysis tasks (see Table 1). Using LLM-based classifiers validated against expert annotations, we labeled queries for structural references, value references, and container references. Figure 1 presents the distribution of data-privileged queries across these datasets.</p> <table> <thead> <tr> <th><strong>Dataset</strong></th> <th style="text-align: center"><strong>Task</strong></th> <th style="text-align: center"><strong>Open-Domain</strong></th> <th style="text-align: right"><strong>#Queries</strong></th> </tr> </thead> <tbody> <tr> <td>WikiTableQuestions<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">16</a></sup></td> <td style="text-align: center">Single Table QA</td> <td style="text-align: center">❌</td> <td style="text-align: right">14,151</td> </tr> <tr> <td>TabMWP<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">17</a></sup></td> <td style="text-align: center">Single Table QA</td> <td style="text-align: center">✅</td> <td style="text-align: right">38,901</td> </tr> <tr> <td>CRT-QA<sup id="fnref:20" role="doc-noteref"><a href="#fn:20" class="footnote" rel="footnote">18</a></sup></td> <td style="text-align: center">Single Table QA</td> <td style="text-align: center">✅</td> <td style="text-align: right">728</td> </tr> <tr> <td>HiTab<sup id="fnref:10:1" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">8</a></sup></td> <td style="text-align: center">Single Table QA</td> <td style="text-align: center">✅</td> <td style="text-align: right">10,672</td> </tr> <tr> <td>OpenWikiTables<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">19</a></sup></td> <td style="text-align: center">Single Table QA</td> <td style="text-align: center">✅</td> <td style="text-align: right">67,023</td> </tr> <tr> <td>OTT-QA<sup id="fnref:9:1" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">5</a></sup></td> <td style="text-align: center">Single Table QA</td> <td style="text-align: center">✅</td> <td style="text-align: right">4,372</td> </tr> <tr> <td>FeTaQA<sup id="fnref:16:1" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">4</a></sup></td> <td style="text-align: center">Single Table QA</td> <td style="text-align: center">✅</td> <td style="text-align: right">10,330</td> </tr> <tr> <td>TableBench<sup id="fnref:19:1" role="doc-noteref"><a href="#fn:19" class="footnote" rel="footnote">12</a></sup></td> <td style="text-align: center">Single Table QA</td> <td style="text-align: center">❌</td> <td style="text-align: right">886</td> </tr> <tr> <td>QTSumm<sup id="fnref:21" role="doc-noteref"><a href="#fn:21" class="footnote" rel="footnote">20</a></sup></td> <td style="text-align: center">Multi Table QA</td> <td style="text-align: center">❌</td> <td style="text-align: right">10,440</td> </tr> <tr> <td>MMQA<sup id="fnref:18:1" role="doc-noteref"><a href="#fn:18" class="footnote" rel="footnote">13</a></sup></td> <td style="text-align: center">Multi Table QA</td> <td style="text-align: center">✅</td> <td style="text-align: right">3,313</td> </tr> <tr> <td>Spider<sup id="fnref:4:1" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">14</a></sup></td> <td style="text-align: center">Text-to-SQL</td> <td style="text-align: center">❌</td> <td style="text-align: right">11,840</td> </tr> <tr> <td>BIRD<sup id="fnref:5:2" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">11</a></sup></td> <td style="text-align: center">Text-to-SQL</td> <td style="text-align: center">❌</td> <td style="text-align: right">10,962</td> </tr> <tr> <td>DA-Code<sup id="fnref:12:1" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">15</a></sup></td> <td style="text-align: center">Tabular Analysis</td> <td style="text-align: center">❌</td> <td style="text-align: right">500</td> </tr> <tr> <td>KramaBench<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">21</a></sup></td> <td style="text-align: center">Tabular Analysis</td> <td style="text-align: center">~</td> <td style="text-align: right">104</td> </tr> <tr> <td>DA-Eval<sup id="fnref:11:1" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">9</a></sup></td> <td style="text-align: center">Tabular Analysis</td> <td style="text-align: center">❌</td> <td style="text-align: right">257</td> </tr> </tbody> </table> <p><strong>Table 1:</strong> Overview of analyzed datasets and their characteristics.</p> <p>The most striking pattern is the correlation between task complexity and data-privilege rates. Complex tabular analysis benchmarks show dramatically higher data-privilege: DA-Eval <sup id="fnref:11:2" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">9</a></sup> at 70% (dominated by structural references at 63%) and DA-Code at 59% (primarily container references)<sup id="fnref:12:2" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">15</a></sup>. In contrast, some simpler question-answering datasets achieve much better data-independence, with FeTaQA <sup id="fnref:16:2" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">4</a></sup> at just 0.4%, OTT-QA <sup id="fnref:9:2" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">5</a></sup> at 5.4%, and HiTab<sup id="fnref:10:2" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">8</a></sup> at 9.6%. This makes intuitive sense, specifying complex analytical operations without referencing specific data structures is genuinely harder. We cannot properly assess systems’ abilities to translate complex insight needs into appropriate analyses if our test queries already encode privileged knowledge about data organization.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/data_privilege_by_dataset-480.webp 480w,/assets/img/blog/data_privilege_by_dataset-800.webp 800w,/assets/img/blog/data_privilege_by_dataset-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/data_privilege_by_dataset.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 1:</strong> Distribution of data-privileged queries across 15 tabular benchmarks, broken down by reference type.</p> <p>A critical finding for the community concerns text-to-SQL datasets. Spider<sup id="fnref:4:2" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">14</a></sup> and BIRD <sup id="fnref:5:3" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">11</a></sup>, while originally designed for closed-domain scenarios, are increasingly being repurposed to evaluate open-domain text-to-SQL systems<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Yet our analysis reveals that 27% and 26% of their queries respectively contain data-privileged information<sup id="fnref:4:3" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">14</a></sup> <sup id="fnref:5:4" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">11</a></sup>. While these rates are more moderate than some of the complex analysis benchmarks, they represent hundreds of queries that provide unrealistic shortcuts in open-domain settings. Researchers adapting these datasets should be aware that a substantial fraction of queries assume knowledge users wouldn’t have when querying unknown databases. Even among datasets explicitly intended for open-domain evaluation, design choices matter: OpenWikiTables, despite its open-domain goals, shows 32% data-privilege<sup id="fnref:13:1" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">19</a></sup>, likely because their methodology for decontextualization with language models insufficiently accounts for the leakage of privileged information.</p> <h2 id="beyond-data-privilege---aligning-queries-with-users-mental-model">Beyond Data-Privilege - Aligning Queries with Users’ Mental Model</h2> <p>Data-privileged queries are actually just one manifestation the broader challenge of aligning queries with users’ mental models when interacting with open-domain systems<sup id="fnref:8:1" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">10</a></sup>. While queries should be data-independent, the queries only make sense if they specify a data scope, that is, when they provide information on what sort of data a query is targeting <sup id="fnref:22" role="doc-noteref"><a href="#fn:22" class="footnote" rel="footnote">22</a></sup>. In a closed-domain setting, the data context itself provides contextual scaffolding that grounds the data boundaries of a query.</p> <p>Consider this query from Spider: <em>“What clubs are there?”</em><sup id="fnref:4:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">14</a></sup> No structural references, no value references, no container references—yet it’s nonsensical in open-domain. What clubs? Where? A real user would specify the data scope, for instance to “What soccer clubs are there in Manchester?”. The query only works because Spider provides a closed database where “clubs” has unambiguous meaning.</p> <p>Closed-domain settings provide enormous implicit information. The database context itself bounds the scope and resolves ambiguities. Open-domain systems eliminate this scaffolding. Users must either explicitly specify data scope or rely on systems to make reasonable inferences, a fundamentally different kind of completeness than closed-domain queries require. For a detailed framework around this division of labor between users and systems in grounding queries, see <a href="https://arxiv.org/abs/2511.04584">our full paper</a>.</p> <h2 id="looking-forward-toward-realistic-open-domain-evaluation">Looking Forward: Toward Realistic Open-Domain Evaluation</h2> <p>The prevalence of data-privilege across popular benchmarks demands that we adapt existing datasets and fundamentally rethinking how we construct new ones, when we want to evaluate open-domain systems.</p> <p>If we want to use existing datasets, we should adapt them thoroughly to the open-domain setting. Problematic queries should be adapted by removing privileged information while ensuring they retain sufficient data scope specification to enable realistic retrieval. This requires careful manual review to maintain the query’s intent while aligning it with users’ natural mental models. For datasets where such adaptation isn’t feasible, researchers should at minimum document data-privilege rates and consider whether the benchmark appropriately tests their evaluation objectives.</p> <p>An interesting direction for constructing new datasets is to shift the paradigm from data-first to query-first methodologies. Current practices—starting with tables and deriving queries from them—has a high likelihood of leaking structural knowledge into the queries themselves. Instead, we should investigate collection approaches that mirror realistic user workflows, gathering information needs from domain experts or users who haven’t seen the underlying data, then identifying relevant tables to satisfy those needs. This inverts the construction process to match how open-domain systems actually operate. Such methodologies are more resource-intensive but essential for benchmarks that are truly aligned with realistic use, truly testing open-domain capabilities.</p> <p>Most critically, the research community must develop awareness of what our benchmarks actually measure. When adapting closed-domain datasets for open-domain evaluation, we cannot simply assume the queries transfer appropriately. When claiming to evaluate table retrieval or open-domain analysis capabilities, we must verify that our test queries don’t provide unrealistic shortcuts that bypass the very capabilities we’re trying to assess. The queries we use fundamentally shape what capabilities systems develop. Aligning systems with many of our current benchmarks may be teaching them to exploit privileged information rather than to genuinely understand user information needs and discover relevant data.</p> <hr/> <p>If you found this blog post interesting have a look at our full paper: <a href="https://arxiv.org/abs/2511.04584">Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data</a></p> <p>Cite as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{gommAreWeAsking2025,
  title = {Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis},
  shorttitle = {Are We Asking the Right Questions?},
  booktitle = {AI for Tabular Data Workshop at EurIPS 2025},
  author = {Gomm, Daniel and Wolff, Cornelius and Hulsebos, Madelon},
  year = 2025,
  url = {https://arxiv.org/abs/2511.04584}
}

</code></pre></div></div> <hr/> <h2 id="references">References</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>X. Zhang, D. Wang, L. Dou, Q. Zhu, and W. Che, “MURRE: Multi-Hop Table Retrieval with Removal for Open-Domain Text-to-SQL,” in <em>Proceedings of the 31st International Conference on Computational Linguistics</em>, O. Rambow, L. Wanner, M. Apidianaki, H. Al-Khalifa, B. D. Eugenio, and S. Schockaert, Eds., Abu Dhabi, UAE: Association for Computational Linguistics, Jan. 2025, pp. 5789–5806. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:2" role="doc-endnote"> <p>M. Kothyari, D. Dhingra, S. Sarawagi, and S. Chakrabarti, “CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL,” in <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, H. Bouamor, J. Pino, and K. Bali, Eds., Singapore: Association for Computational Linguistics, Dec. 2023, pp. 14054–14066. doi: <a href="https://doi.org/10.18653/v1/2023.emnlp-main.868">10.18653/v1/2023.emnlp-main.868</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:7" role="doc-endnote"> <p>J. Herzig, T. Müller, S. Krichene, and J. Eisenschlos, “Open Domain Question Answering over Tables via Dense Retrieval,” in <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, K. Toutanova <em>et al.</em>, Eds., Online: Association for Computational Linguistics, Jun. 2021, pp. 512–519. doi: <a href="https://doi.org/10.18653/v1/2021.naacl-main.43">10.18653/v1/2021.naacl-main.43</a>. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:16" role="doc-endnote"> <p>L. Nan <em>et al.</em>, “FeTaQA: Free-form Table Question Answering,” <em>Transactions of the Association for Computational Linguistics</em>, vol. 10, pp. 35–49, 2022, doi: <a href="https://doi.org/10.1162/tacl_a_00446">10.1162/tacl_a_00446</a>. <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:16:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:16:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> <li id="fn:9" role="doc-endnote"> <p>W. Chen, M.-W. Chang, E. Schlinger, W. Y. Wang, and W. W. Cohen, “Open Question Answering over Tables and Text,” presented at the International Conference on Learning Representations, Oct. 2020. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:9:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:9:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> <li id="fn:6" role="doc-endnote"> <p>J. Wang and G. Li, “AOP: Automated and Interactive LLM Pipeline Orchestration for Answering Complex Queries,” presented at the Conference on Innovative Database Research, 2025. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>K. Kong <em>et al.</em>, “OpenTab: Advancing Large Language Models as Open-domain Table Reasoners,” presented at the The Twelfth International Conference on Learning Representations, Oct. 2023. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:10" role="doc-endnote"> <p>Z. Cheng <em>et al.</em>, “HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation,” in <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, S. Muresan, P. Nakov, and A. Villavicencio, Eds., Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 1094–1110. doi: <a href="https://doi.org/10.18653/v1/2022.acl-long.78">10.18653/v1/2022.acl-long.78</a>. <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:10:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:10:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> <li id="fn:11" role="doc-endnote"> <p>X. Hu <em>et al.</em>, “InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks,” in <em>Proceedings of the 41st International Conference on Machine Learning</em>, PMLR, Jul. 2024, pp. 19544–19572. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:11:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:11:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> <li id="fn:8" role="doc-endnote"> <p>D. A. Norman, “Some Observations on Mental Models,” in <em>Mental Models</em>, Psychology Press, 1983. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:8:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:5" role="doc-endnote"> <p>J. Li <em>et al.</em>, “Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,” in <em>Advances in Neural Information Processing Systems</em>, 2024. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:5:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:5:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:5:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a></p> </li> <li id="fn:19" role="doc-endnote"> <p>X. Wu <em>et al.</em>, “TableBench: A Comprehensive and Complex Benchmark for Table Question Answering,” Mar. 18, 2025, <em>arXiv</em>: arXiv:2408.09174. doi: <a href="https://doi.org/10.48550/arXiv.2408.09174">10.48550/arXiv.2408.09174</a>. <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:19:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:18" role="doc-endnote"> <p>J. Wu, L. Yang, D. Li, Y. Ji, M. Okumura, and Y. Zhang, “MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions,” presented at the The Thirteenth International Conference on Learning Representations, Oct. 2024. <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:18:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:4" role="doc-endnote"> <p>T. Yu <em>et al.</em>, “Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task,” Feb. 02, 2019, <em>arXiv</em>: arXiv:1809.08887. doi: <a href="https://doi.org/10.48550/arXiv.1809.08887">10.48550/arXiv.1809.08887</a>. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:4:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:4:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:4:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a></p> </li> <li id="fn:12" role="doc-endnote"> <p>Y. Huang <em>et al.</em>, “DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models,” in <em>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, Association for Computational Linguistics, Oct. 2024. doi: <a href="https://doi.org/10.18653/v1/2024.emnlp-main.748">10.18653/v1/2024.emnlp-main.748</a>. <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:12:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:12:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> <li id="fn:17" role="doc-endnote"> <p>P. Pasupat and P. Liang, “Compositional Semantic Parsing on Semi-Structured Tables,” in <em>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, C. Zong and M. Strube, Eds., Beijing, China: Association for Computational Linguistics, Jul. 2015, pp. 1470–1480. doi: <a href="https://doi.org/10.3115/v1/P15-1142">10.3115/v1/P15-1142</a>. <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:15" role="doc-endnote"> <p>P. Lu <em>et al.</em>, “Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning,” presented at the The Eleventh International Conference on Learning Representations, Sep. 2022. <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:20" role="doc-endnote"> <p>Z. Zhang, X. Li, Y. Gao, and J.-G. Lou, “CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data,” in <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, H. Bouamor, J. Pino, and K. Bali, Eds., Singapore: Association for Computational Linguistics, Dec. 2023, pp. 2131–2153. doi: <a href="https://doi.org/10.18653/v1/2023.emnlp-main.132">10.18653/v1/2023.emnlp-main.132</a>. <a href="#fnref:20" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:13" role="doc-endnote"> <p>S. Kweon, Y. Kwon, S. Cho, Y. Jo, and E. Choi, “Open-WikiTable : Dataset for Open Domain Question Answering with Complex Reasoning over Table,” in <em>Findings of the Association for Computational Linguistics: ACL 2023</em>, A. Rogers, J. Boyd-Graber, and N. Okazaki, Eds., Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 8285–8297. doi: <a href="https://doi.org/10.18653/v1/2023.findings-acl.526">10.18653/v1/2023.findings-acl.526</a>. <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:13:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:21" role="doc-endnote"> <p>Y. Zhao, Y. Li, C. Li, and R. Zhang, “MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data,” in <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, S. Muresan, P. Nakov, and A. Villavicencio, Eds., Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 6588–6600. doi: <a href="https://doi.org/10.18653/v1/2022.acl-long.454">10.18653/v1/2022.acl-long.454</a>. <a href="#fnref:21" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:14" role="doc-endnote"> <p>E. Lai <em>et al.</em>, “KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes,” Jun. 06, 2025, <em>arXiv</em>: arXiv:2506.06541. doi: <a href="https://doi.org/10.48550/arXiv.2506.06541">10.48550/arXiv.2506.06541</a>. <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:22" role="doc-endnote"> <p>D. Gomm and M. Hulsebos, “Metadata Matters in Dense Table Retrieval,” in <em>ELLIS workshop on Representation Learning and Generative Models for Structured Data</em>, Feb. 2025. <a href="#fnref:22" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Daniel Gomm</name></author><category term="article"/><category term="trl"/><category term="open-domain"/><category term="benchmarks"/><summary type="html"><![CDATA[Investigation into what information is leaked into queries in popular tabular benchmarks.]]></summary></entry><entry><title type="html">A Practical Guide to the OpenAI Batch API with Python and openbatch</title><link href="https://daniel-gomm.github.io/blog/2025/openbatch/" rel="alternate" type="text/html" title="A Practical Guide to the OpenAI Batch API with Python and openbatch"/><published>2025-09-29T00:00:00+00:00</published><updated>2025-09-29T00:00:00+00:00</updated><id>https://daniel-gomm.github.io/blog/2025/openbatch</id><content type="html" xml:base="https://daniel-gomm.github.io/blog/2025/openbatch/"><![CDATA[<blockquote> <p>TL;DR The OpenAI Batch API offers a <strong>50% cost reduction</strong> and significantly higher throughput for large-scale tasks, but its file-based workflow is cumbersome, especially for structured outputs. The <a href="https://daniel-gomm.github.io/openbatch"><code class="language-plaintext highlighter-rouge">openbatch</code></a> Python library eliminates this friction by providing a convenient, <strong>drop-in replacement for the standard OpenAI client</strong>. It simplifies batch file creation, streamlines structured data handling using Pydantic, and offers powerful templating, making the cost and speed benefits of batch processing easily accessible without sacrificing developer convenience.</p> </blockquote> <p>For researchers and developers working with large datasets, the <a href="https://platform.openai.com/docs/guides/batch/batch-api">OpenAI Batch API</a> offers significant advantages in cost and speed. However, its asynchronous, file-based workflow can feel cumbersome compared to the simplicity of direct API calls. This guide explores the trade-offs and introduces <a href="https://daniel-gomm.github.io/openbatch">openbatch</a>, a Python package designed to make the Batch API as convenient to use as the standard sequential API.</p> <h2 id="batch-processing-is-cheaper-and-often-faster">Batch Processing is Cheaper and often Faster</h2> <p>The primary motivation for using the Batch API is efficiency. It provides two key benefits:</p> <ol> <li><strong>50% Cost Reduction</strong>: Batch API calls are priced at <strong>half the cost</strong> of the standard API. For large-scale data analysis, classification, or generation tasks, this immediately doubles your budget’s effectiveness.</li> <li><strong>High Throughput</strong>: While batch jobs have a 24-hour completion window, they often finish much faster than an equivalent number of sequential calls. In one experiment, a task that would have taken over <strong>10 hours of sequential API calls</strong> was <strong>completed in under 1 hour</strong> using a single batch job.</li> </ol> <p>The trade-off for this (cost-)efficiency has traditionally been convenience. Instead of a simple request-response cycle, the batch workflow involves manually preparing a JSONL file, uploading it, starting the job, and then retrieving the results from a separate file. This is especially cumbersome and challenging when you need to generate prompts dynamically or make use of <a href="https://platform.openai.com/docs/guides/structured-outputs">structured outputs</a>.</p> <h2 id="convenient-batch-processing-with-openbatch">Convenient Batch Processing with <code class="language-plaintext highlighter-rouge">openbatch</code></h2> <p><code class="language-plaintext highlighter-rouge">openbatch</code> is a lightweight Python library that simplifies the creation of the batch input file by providing a developer experience that mirrors the official <code class="language-plaintext highlighter-rouge">openai</code> Python client. It’s designed to be a near drop-in replacement that can integrate into existing workflows, allowing you to switch between sequential and batch processing with minimal code changes.</p> <p>The library’s core features directly address the common pain points of the batch workflow:</p> <ul> <li><strong>Familiar API</strong>: The <code class="language-plaintext highlighter-rouge">BatchCollector</code> class mimics the structure of <code class="language-plaintext highlighter-rouge">openai.OpenAI</code>, so you can write <code class="language-plaintext highlighter-rouge">collector.responses.create(...)</code> instead of <code class="language-plaintext highlighter-rouge">client.responses.create(...)</code>.</li> <li><strong>Structured Outputs with Pydantic</strong>: Reliably getting JSON output from LLMs can be tricky. <code class="language-plaintext highlighter-rouge">openbatch</code> allows you to pass a Pydantic model directly to a <code class="language-plaintext highlighter-rouge">parse()</code> method (e.g., <code class="language-plaintext highlighter-rouge">collector.responses.parse(text_format=MyModel)</code>), which automatically handles the complex JSON schema generation needed to enforce the output structure.</li> <li><strong>Powerful Templating</strong>: With the <code class="language-plaintext highlighter-rouge">BatchJobManager</code>, you can define a <code class="language-plaintext highlighter-rouge">PromptTemplate</code> with placeholders and programmatically generate thousands or millions of requests from a list of data, which is ideal for large-scale, repetitive tasks.</li> <li><strong>Full API Coverage</strong>: It supports all endpoints available in the Batch API: <code class="language-plaintext highlighter-rouge">/v1/responses</code>, <code class="language-plaintext highlighter-rouge">/v1/chat/completions</code>, and <code class="language-plaintext highlighter-rouge">/v1/embeddings</code>.</li> </ul> <p>Have a look at the full documentation at <a href="https://daniel-gomm.github.io/openbatch">https://daniel-gomm.github.io/openbatch</a> for more details and how-to guides.</p> <h3 id="installation">Installation</h3> <p>You can install <code class="language-plaintext highlighter-rouge">openbatch</code> via pip:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>openbatch
</code></pre></div></div> <h3 id="a-practical-workflow-sentiment-analysis-at-scale">A Practical Workflow: Sentiment Analysis at Scale</h3> <p>To understand the value of <code class="language-plaintext highlighter-rouge">openbatch</code>, let’s walk through a common, real-world task: performing sentiment analysis on a large dataset of customer reviews. Imagine you have a file, <code class="language-plaintext highlighter-rouge">customer_reviews.csv</code>, containing thousands of reviews that you need to classify.</p> <p>The goal is to get a structured output for each review, classifying it as ‘Positive’, ‘Neutral’, or ‘Negative’ with a confidence score. We’ll start with a standard sequential approach and then see how <code class="language-plaintext highlighter-rouge">openbatch</code> dramatically simplifies the process for batching.</p> <p>First, let’s define the Pydantic model that will enforce our desired output structure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Literal</span>

<span class="k">class</span> <span class="nc">SentimentAnalysisModel</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">sentiment</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="sh">"</span><span class="s">Positive</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Neutral</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Negative</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">confidence</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span>
        <span class="n">ge</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">le</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">Confidence score for the sentiment classification.</span><span class="sh">"</span>
    <span class="p">)</span>
</code></pre></div></div> <p>Now, let’s assume our <code class="language-plaintext highlighter-rouge">customer_reviews.csv</code> looks like this:</p> <table> <thead> <tr> <th>review_id</th> <th>review_text</th> </tr> </thead> <tbody> <tr> <td>a-123</td> <td>The product is absolutely fantastic!</td> </tr> <tr> <td>b-456</td> <td>It broke after just one week of use.</td> </tr> <tr> <td>c-789</td> <td>The packaging was okay.</td> </tr> <tr> <td>…</td> <td><em>(thousands more rows)</em></td> </tr> </tbody> </table> <h4 id="approach-1-the-standard-sequential-loop">Approach 1: The Standard Sequential Loop</h4> <p>A typical approach would be to loop through the dataset and make an API call for each review. This is easy to write but highly inefficient for large datasets.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">csv</span>
<span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">instructions</span> <span class="o">=</span> <span class="sh">"</span><span class="s">You are an expert annotator. Judge the sentiment of the user-provided comment. Use the categories </span><span class="sh">'</span><span class="s">Positive</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">Neutral</span><span class="sh">'</span><span class="s">, and </span><span class="sh">'</span><span class="s">Negative</span><span class="sh">'</span><span class="s">.</span><span class="sh">"</span>

<span class="c1"># This loop is slow and expensive at scale
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Starting sequential processing...</span><span class="sh">"</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">customer_reviews.csv</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="nc">DictReader</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">responses</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">instructions</span><span class="o">=</span><span class="n">instructions</span><span class="p">,</span>
            <span class="nb">input</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_text</span><span class="sh">"</span><span class="p">]}],</span>
            <span class="n">text_format</span><span class="o">=</span><span class="n">SentimentAnalysisModel</span>
        <span class="p">)</span>
        <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">review_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_id</span><span class="sh">"</span><span class="p">],</span> <span class="sh">"</span><span class="s">analysis</span><span class="sh">"</span><span class="p">:</span> <span class="n">response</span><span class="p">})</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Processed review </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">review_id</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">✅ Sequential processing complete.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This method is simple but has major drawbacks: it’s <strong>slow</strong>, processing one review at a time, and <strong>expensive</strong>, as it uses standard API pricing.</p> <h4 id="approach-2-the-batchcollector-drop-in-replacement">Approach 2: The <code class="language-plaintext highlighter-rouge">BatchCollector</code> Drop-in Replacement</h4> <p>Here’s where <code class="language-plaintext highlighter-rouge">openbatch</code> comes in. You can switch to batch processing with minimal changes. The <code class="language-plaintext highlighter-rouge">BatchCollector</code> API is designed to be a <strong>drop-in replacement</strong> for the <code class="language-plaintext highlighter-rouge">OpenAI</code> client within your existing loop.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">csv</span>
<span class="kn">from</span> <span class="n">openbatch</span> <span class="kn">import</span> <span class="n">BatchCollector</span>

<span class="n">batch_file_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sentiment_batch.jsonl</span><span class="sh">"</span>
<span class="n">collector</span> <span class="o">=</span> <span class="nc">BatchCollector</span><span class="p">(</span><span class="n">batch_file_path</span><span class="o">=</span><span class="n">batch_file_path</span><span class="p">)</span> <span class="c1"># Change 1: Instantiate collector
</span><span class="n">instructions</span> <span class="o">=</span> <span class="sh">"</span><span class="s">You are an expert annotator. Judge the sentiment of the user-provided comment. Use the categories </span><span class="sh">'</span><span class="s">Positive</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">Neutral</span><span class="sh">'</span><span class="s">, and </span><span class="sh">'</span><span class="s">Negative</span><span class="sh">'</span><span class="s">.</span><span class="sh">"</span>

<span class="c1"># The same loop, now preparing a batch file instead of making live calls
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Preparing batch file with BatchCollector...</span><span class="sh">"</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">customer_reviews.csv</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="nc">DictReader</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
        <span class="c1"># Change 2: Call the collector instead of the client
</span>        <span class="n">collector</span><span class="p">.</span><span class="n">responses</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span>
            <span class="n">custom_id</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_id</span><span class="sh">"</span><span class="p">],</span> <span class="c1"># Add a custom ID for tracking
</span>            <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">instructions</span><span class="o">=</span><span class="n">instructions</span><span class="p">,</span>
            <span class="nb">input</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_text</span><span class="sh">"</span><span class="p">]}],</span>
            <span class="n">text_format</span><span class="o">=</span><span class="n">SentimentAnalysisModel</span>
        <span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">✅ Batch file </span><span class="sh">'</span><span class="si">{</span><span class="n">batch_file_path</span><span class="si">}</span><span class="sh">'</span><span class="s"> prepared.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>With just two lines changed, your code now prepares a cost-effective batch job instead of making slow, expensive API calls. You’ve decoupled file preparation from execution.</p> <h4 id="approach-3-the-simplified-batchjobmanager">Approach 3: The Simplified <code class="language-plaintext highlighter-rouge">BatchJobManager</code></h4> <p>For templated, large-scale tasks like this, the <code class="language-plaintext highlighter-rouge">BatchJobManager</code> is even more efficient. It eliminates the need for an explicit Python loop entirely, letting you define the task and the data separately.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">csv</span>
<span class="kn">from</span> <span class="n">openbatch</span> <span class="kn">import</span> <span class="n">BatchJobManager</span><span class="p">,</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">Message</span><span class="p">,</span> <span class="n">ResponsesRequest</span><span class="p">,</span> <span class="n">PromptTemplateInputInstance</span>

<span class="c1"># 1. Define the prompt template
</span><span class="n">template</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[</span>
    <span class="nc">Message</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="sh">"</span><span class="s">You are an expert annotator. Judge the sentiment of the user-provided comment. Use the categories </span><span class="sh">'</span><span class="s">Positive</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">Neutral</span><span class="sh">'</span><span class="s">, and </span><span class="sh">'</span><span class="s">Negative</span><span class="sh">'</span><span class="s">.</span><span class="sh">"</span><span class="p">),</span>
    <span class="nc">Message</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="sh">"</span><span class="s">{review_text}</span><span class="sh">"</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># 2. Define the common request configuration
</span><span class="n">common_config</span> <span class="o">=</span> <span class="nc">ResponsesRequest</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Enforce the structured output for all requests in this batch
</span><span class="n">common_config</span><span class="p">.</span><span class="nf">set_output_structure</span><span class="p">(</span><span class="n">SentimentAnalysisModel</span><span class="p">)</span>

<span class="c1"># 3. Load the data and create input instances (no loop needed for API calls)
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">customer_reviews.csv</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="nc">DictReader</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">input_instances</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nc">PromptTemplateInputInstance</span><span class="p">(</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_id</span><span class="sh">"</span><span class="p">],</span>
            <span class="n">prompt_value_mapping</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">review_text</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">review_text</span><span class="sh">"</span><span class="p">]}</span>
        <span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span>
    <span class="p">]</span>

<span class="c1"># 4. Generate the entire batch file in one go
</span><span class="n">batch_file_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sentiment_batch_optimized.jsonl</span><span class="sh">"</span>
<span class="n">manager</span> <span class="o">=</span> <span class="nc">BatchJobManager</span><span class="p">()</span>
<span class="n">manager</span><span class="p">.</span><span class="nf">add_templated_instances</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">template</span><span class="p">,</span>
    <span class="n">common_request</span><span class="o">=</span><span class="n">common_config</span><span class="p">,</span>
    <span class="n">input_instances</span><span class="o">=</span><span class="n">input_instances</span><span class="p">,</span>
    <span class="n">save_file_path</span><span class="o">=</span><span class="n">batch_file_path</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Batch file </span><span class="sh">'</span><span class="si">{</span><span class="n">batch_file_path</span><span class="si">}</span><span class="sh">'</span><span class="s"> generated with BatchJobManager.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This approach is the cleanest and most declarative, perfectly suited for large-scale, programmatic batch job creation.</p> <h4 id="creating-the-batch-job">Creating the Batch Job</h4> <p>Once the batch-job-file is created, the batch-job can be started. This is possible through the <a href="https://platform.openai.com/batches">Batches Web UI</a> or programatically via the standard <code class="language-plaintext highlighter-rouge">openai</code> client:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">()</span>

<span class="c1"># Upload the file to OpenAI
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Uploading </span><span class="sh">'</span><span class="si">{</span><span class="n">batch_file_path</span><span class="si">}</span><span class="sh">'</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">batch_input_file</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">files</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="nb">file</span><span class="o">=</span><span class="nf">open</span><span class="p">(</span><span class="n">batch_file_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">purpose</span><span class="o">=</span><span class="sh">"</span><span class="s">batch</span><span class="sh">"</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">File uploaded. File ID: </span><span class="si">{</span><span class="n">batch_input_file</span><span class="p">.</span><span class="nb">id</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Create the batch job, specifying the correct endpoint
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Creating batch job...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">batch_job</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">batches</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">input_file_id</span><span class="o">=</span><span class="n">batch_input_file</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="sh">"</span><span class="s">/v1/responses</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Must match the API used in your requests
</span>    <span class="n">completion_window</span><span class="o">=</span><span class="sh">"</span><span class="s">24h</span><span class="sh">"</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Batch job created. Job ID: </span><span class="si">{</span><span class="n">batch_job</span><span class="p">.</span><span class="nb">id</span><span class="si">}</span><span class="s">, Status: </span><span class="si">{</span><span class="n">batch_job</span><span class="p">.</span><span class="n">status</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h4 id="monitor-and-download-the-results">Monitor and Download the Results</h4> <p>After some time, you can check the job’s status. Once it’s complete, you can download the output file containing the responses. This is again possible through the <a href="https://platform.openai.com/batches">Batches Web UI</a> or programatically:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In a real application, you could poll this endpoint periodically
</span><span class="n">completed_job</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">batches</span><span class="p">.</span><span class="nf">retrieve</span><span class="p">(</span><span class="n">batch_job</span><span class="p">.</span><span class="nb">id</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Current job status: </span><span class="si">{</span><span class="n">completed_job</span><span class="p">.</span><span class="n">status</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Once the status is 'completed'
</span><span class="k">if</span> <span class="n">completed_job</span><span class="p">.</span><span class="n">status</span> <span class="o">==</span> <span class="sh">'</span><span class="s">completed</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">output_file_id</span> <span class="o">=</span> <span class="n">completed_job</span><span class="p">.</span><span class="n">output_file_id</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Job finished! Output file ID: </span><span class="si">{</span><span class="n">output_file_id</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Download the results file
</span>    <span class="n">results_content</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">files</span><span class="p">.</span><span class="nf">content</span><span class="p">(</span><span class="n">file_id</span><span class="o">=</span><span class="n">output_file_id</span><span class="p">)</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">batch_results.jsonl</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">results_content</span><span class="p">.</span><span class="nf">read</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">✅ Results saved to </span><span class="sh">'</span><span class="s">batch_results.jsonl</span><span class="sh">'</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">completed_job</span><span class="p">.</span><span class="n">status</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">failed</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">expired</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">cancelled</span><span class="sh">'</span><span class="p">]:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">❌ Job did not complete. Status: </span><span class="si">{</span><span class="n">completed_job</span><span class="p">.</span><span class="n">status</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>The downloaded <code class="language-plaintext highlighter-rouge">batch_results.jsonl</code> file will contain the output for each of the input requests, which can then be parsed for the application. Thanks to the structured output feature, the JSON responses for the requests will be clean, valid, and ready to be loaded directly into the Pydantic models.</p> <h2 id="conclusion">Conclusion</h2> <p>The OpenAI Batch API is a powerful tool for processing large datasets efficiently and cost-effectively. While its asynchronous nature can introduce complexity, libraries like <a href="https://daniel-gomm.github.io/openbatch"><code class="language-plaintext highlighter-rouge">openbatch</code></a> abstract away the tedious work of file preparation. By providing a familiar, Pydantic-powered interface, it makes the benefits of batch processing accessible without sacrificing developer convenience, allowing researchers and engineers to focus on their results, not their boilerplate.</p> <p>For more detail on <code class="language-plaintext highlighter-rouge">openbatch</code>, including installation instructions and comprehensive documentation, visit the <a href="https://daniel-gomm.github.io/openbatch">official documentation site</a>.</p>]]></content><author><name></name></author><category term="project"/><category term="code"/><category term="guide"/><category term="openai"/><summary type="html"><![CDATA[This post introduces `openbatch`, a Python library designed to make the powerful but often cumbersome OpenAI Batch API as convenient and easy to use as standard sequential calls.]]></summary></entry><entry><title type="html">Table Serialization Kitchen - A Recipe for Better LLM Performance on Tabular Data</title><link href="https://daniel-gomm.github.io/blog/2025/Table-Serialization-Kitchen/" rel="alternate" type="text/html" title="Table Serialization Kitchen - A Recipe for Better LLM Performance on Tabular Data"/><published>2025-03-04T00:00:00+00:00</published><updated>2025-03-04T00:00:00+00:00</updated><id>https://daniel-gomm.github.io/blog/2025/Table%20Serialization%20Kitchen</id><content type="html" xml:base="https://daniel-gomm.github.io/blog/2025/Table-Serialization-Kitchen/"><![CDATA[<blockquote> <p>TL;DR: This blog post explores the importance of table serialization on the performance of Large Language Models. We introduce the <a href="https://github.com/daniel-gomm/table-serialization-kitchen">Table Serialization Kitchen</a>, an easy-to-use open-source tool for experimenting with serialization strategies. Initial experiments in table retrieval show large performance differences between different serializations. Metadata can provide highly relevant signal and improve performance significantly. Besides this, we find that there exist no single best serialization strategy but rather that the serialization strategy has to be tailored to the embedding model. Overall, our findings suggest that by optimizing table serialization, you can significantly enhance LLM performance on tabular data.</p> </blockquote> <p>Large Language Models (LLMs) have revolutionized how we interact with data, enabling tasks like question answering, text-to-SQL generation, and more. However, these models typically require text-based inputs, which poses a challenge when working with tabular data. To apply LLMs to tables, we need to convert the structured data into a textual format—a process known as <strong>table serialization</strong>. But how do we serialize tables effectively? And does the way we serialize them impact the performance of downstream tasks?</p> <p>In this blog post, we dive into the world of table serialization for dense retrieval tasks. We’ll share our journey of experimenting with different serialization strategies, highlighting key findings and providing practical insights. Along the way, we’ll introduce the <a href="https://github.com/daniel-gomm/table-serialization-kitchen"><strong>Table Serialization Kitchen</strong></a>, an open-source tool we developed to make experimentation easier. (This project was presented at the <a href="https://sites.google.com/view/rl-and-gm-for-sd">ELLIS workshop on Representation Learning and Generative Models for Structured Data 2025</a>. Find our extended abstract <a href="https://openreview.net/forum?id=rELWIvq2Qy">“Metadata Matters in Dense Table Retrieval” here</a>)</p> <h2 id="the-challenge-of-table-serialization">The Challenge of Table Serialization</h2> <p>Let’s start with an example. Consider the following table from the FeTaQA<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup> dataset:</p> <table> <thead> <tr> <th>Year</th> <th>Title</th> <th>Role</th> </tr> </thead> <tbody> <tr> <td>2012</td> <td>From the Rough</td> <td>Edward</td> </tr> <tr> <td>1997</td> <td>The Borrowers</td> <td>Peagreen Clock</td> </tr> <tr> <td>2013</td> <td>In Secret</td> <td>Camille Raquin</td> </tr> <tr> <td>2004</td> <td>Harry Potter and the Prisoner of Azkaban</td> <td>Draco Malfoy</td> </tr> <tr> <td>2017</td> <td>Feed</td> <td>Matt Grey</td> </tr> </tbody> </table> <p>This table is also associated with metadata:</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"table_page_title"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Tom Felton"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"table_section_title"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Films"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>To use this table with an LLM, we need to convert it into a textual format. One possible serialization might look like this:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Metadata:
table_page_title: Tom Felton
table_section_title: Films

Table:
| Year | Title          | Role           |
|------|----------------|----------------|
| 2012 | From the Rough | Edward         |
| 1997 | The Borrowers  | Peagreen Clock |
| 2013 | In Secret      | Camille Raquin |
</code></pre></div></div> <p>This representation includes both the table’s content and its metadata, formatted in Markdown. But is this the best way to serialize the table? Should we include all the rows, or just a subset? How can select such a subset of rows? What about the metadata—is it helpful, or just noise? And how does the format (e.g., Markdown vs. JSON) affect the results?</p> <p>These questions highlight the complexity of table serialization. There are countless design choices to make, and it’s not immediately clear which ones lead to better performance. To answer these questions, we turned to experimentation.</p> <h2 id="setting-up-experiments-with-table-serialization-kitchen-and-target">Setting up experiments with Table Serialization Kitchen and TARGET</h2> <p>To systematically explore the design space of table serialization, we developed the <a href="https://github.com/daniel-gomm/table-serialization-kitchen"><strong>Table Serialization Kitchen</strong></a>—a Python package that makes it easy to define, test, and compare different serialization strategies. The package integrates with the <a href="https://target-benchmark.github.io/">TARGET benchmark</a><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, allowing us to evaluate serialization methods for table retrieval on real-world datasets like FeTaQA<sup id="fnref:4:1" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup>, OTTQA<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, Spider<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">4</a></sup>, and BIRD<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">5</a></sup>.</p> <p>To follow along this blog post, install <a href="https://github.com/daniel-gomm/table-serialization-kitchen">Table Serialization Kitchen</a> with the TARGET integration:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>tableserializer[target]
</code></pre></div></div> <p>Since there are so many parameters to explore, and the number of experiments scales exponentially with the number of parameters varied, we fix the row sampling strategy to random selection and we do not perform any preprocessing of the data in the tables. Besides that we explore the following parameter configurations:</p> <ul> <li>Recipes (Serialization templates): We evaluate two serialization templates, one that only data from the raw table itself, and one that also leaves room for metadata.</li> <li>Raw table serialization: We compare serializing raw table contents as row-wise JSON and in markdown format.</li> <li>Row sampling: We investigate the impact of varying the number of rows sampled for the serialization in a range of 1-30 sampled rows.</li> </ul> <p>Table serialization kitchen makes creating configurations for all of these settings straight forward:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tableserializer.kitchen</span> <span class="kn">import</span> <span class="n">ExperimentalSerializerKitchen</span>
<span class="kn">from</span> <span class="n">tableserializer.serializer.table</span> <span class="kn">import</span> <span class="n">MarkdownRawTableSerializer</span><span class="p">,</span> <span class="n">JSONRawTableSerializer</span>
<span class="kn">from</span> <span class="n">tableserializer.serializer.metadata</span> <span class="kn">import</span> <span class="n">PairwiseMetadataSerializer</span>
<span class="kn">from</span> <span class="n">tableserializer.table.row_sampler</span> <span class="kn">import</span> <span class="n">RandomRowSampler</span>
<span class="kn">from</span> <span class="n">tableserializer</span> <span class="kn">import</span> <span class="n">SerializationRecipe</span>


<span class="c1"># Define all the different configurations of components to use in the experiments
</span>
<span class="c1"># Recipes define the general structure of the output
</span><span class="n">recipes</span> <span class="o">=</span> <span class="p">[</span><span class="nc">SerializationRecipe</span><span class="p">(</span><span class="sh">"</span><span class="s">Metadata:</span><span class="se">\n</span><span class="s">{META}</span><span class="se">\n\n</span><span class="s">Table:</span><span class="se">\n</span><span class="s">{TABLE}</span><span class="sh">"</span><span class="p">),</span>
           <span class="nc">SerializationRecipe</span><span class="p">(</span><span class="sh">"</span><span class="s">Table:</span><span class="se">\n</span><span class="s">{TABLE}</span><span class="sh">"</span><span class="p">)]</span>

<span class="c1"># Metadata serializers define how metadata is parsed and serialized
</span><span class="n">metadata_serializers</span> <span class="o">=</span> <span class="p">[</span><span class="nc">PairwiseMetadataSerializer</span><span class="p">()]</span>

<span class="c1"># Raw table serializers define how raw tables are serialized
</span><span class="n">raw_table_serializers</span> <span class="o">=</span> <span class="p">[</span><span class="nc">MarkdownRawTableSerializer</span><span class="p">(),</span> <span class="nc">JSONRawTableSerializer</span><span class="p">()]</span>

<span class="c1"># Row samplers define how and how many rows are sampled for the raw table serialization
# Here we just use random row samplers, sampling 1-30 rows
</span><span class="n">row_samplers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">+</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">5</span><span class="p">)):</span>
    <span class="n">row_samplers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">RandomRowSampler</span><span class="p">(</span><span class="n">rows_to_sample</span><span class="o">=</span><span class="n">i</span><span class="p">))</span>

<span class="c1"># The experimental serializer kitchen is used to manage table serializers for experimentation
</span><span class="n">kitchen</span> <span class="o">=</span> <span class="nc">ExperimentalSerializerKitchen</span><span class="p">()</span>

<span class="c1"># Create an array of serializers with different configurations (grid-search-style combination of all parameters)
</span><span class="n">serializers</span> <span class="o">=</span> <span class="n">kitchen</span><span class="p">.</span><span class="nf">create_serializers</span><span class="p">(</span><span class="n">recipes</span><span class="o">=</span><span class="n">recipes</span><span class="p">,</span>
                                         <span class="n">schema_serializers</span><span class="o">=</span><span class="p">[],</span>
                                         <span class="n">metadata_serializers</span><span class="o">=</span><span class="n">metadata_serializers</span><span class="p">,</span>
                                         <span class="n">table_serializers</span><span class="o">=</span><span class="n">raw_table_serializers</span><span class="p">,</span>
                                         <span class="n">row_samplers</span><span class="o">=</span><span class="n">row_samplers</span><span class="p">,</span>
                                         <span class="n">table_preprocessor_constellations</span><span class="o">=</span><span class="p">[[]])</span>

<span class="c1"># Save the serializers in a folder structure
</span><span class="n">kitchen</span><span class="p">.</span><span class="nf">save_serializer_experiment_configurations</span><span class="p">(</span><span class="n">serializers</span><span class="p">,</span> <span class="sh">"</span><span class="s">./retrieval_experiments</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This creates a folder structure that contains the configurations for all the different combinations of parameters. We can now run these experiments on a dataset (FeTaQA<sup id="fnref:4:2" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup> in this case) using the TARGET integration into table serializer kitchen:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tableserializer.integrations.target</span> <span class="kn">import</span> <span class="n">TARGETOpenAIExperimentExecutor</span>
<span class="kn">from</span> <span class="n">tableserializer.kitchen</span> <span class="kn">import</span> <span class="n">ExperimentalSerializerKitchen</span>

<span class="c1"># Create experiment executor that can run a TARGET experiment
</span><span class="n">experiment_executor</span> <span class="o">=</span> <span class="nc">TARGETOpenAIExperimentExecutor</span><span class="p">(</span><span class="sh">"</span><span class="s">INSERT_API_KEY_HERE</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fetaqa</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">,</span>
                                                     <span class="n">embedding_cache_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">./retrieval_experiments/.cache</span><span class="sh">"</span><span class="p">)</span>

<span class="n">kitchen</span> <span class="o">=</span> <span class="nc">ExperimentalSerializerKitchen</span><span class="p">()</span>

<span class="c1"># Run all experiments in the directory
</span><span class="n">kitchen</span><span class="p">.</span><span class="nf">run_experiments_with_serializers</span><span class="p">(</span><span class="n">base_folder</span><span class="o">=</span><span class="sh">"</span><span class="s">./retrieval_experiments</span><span class="sh">"</span><span class="p">,</span>
                                         <span class="n">experiment_callback</span><span class="o">=</span><span class="n">experiment_executor</span><span class="p">.</span><span class="n">run_experiment</span><span class="p">)</span>
</code></pre></div></div> <h2 id="how-to-serialize-tables-for-table-retrieval">How to serialize tables for table retrieval?</h2> <p>We use TARGET<sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> to benchmark table retrieval with the different serializers. TARGET provide a unified interface to run table retrieval benchmarks on datasets from different tasks like TabularQA (e.g., FeTaQA<sup id="fnref:4:3" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup>, OTTQA<sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>) or Text-to-SQL (Spider<sup id="fnref:5:1" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">4</a></sup>, BIRD<sup id="fnref:6:1" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">5</a></sup>). These experiments show large deviations in retrieval performance depending on how the serializations are constructed.</p> <h3 id="metadata-matters">Metadata Matters</h3> <p>One of the most striking results from our experiments is the significant impact of including contextual metadata in the table serialization. In the FeTaQA<sup id="fnref:4:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup> dataset, tables are associated with page and section titles as metadata. When this metadata is included in the serialized representation, retrieval performance improves substantially.</p> <p>For example, as shown in the plots below, including metadata increases the average recall@3 by <strong>0.42</strong>. This demonstrates that contextualizing tables with relevant metadata is crucial for effective retrieval. The metadata provides additional context that helps the embedding models better understand the relevance of the table to a given query.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/fetaqa_results-480.webp 480w,/assets/img/blog/fetaqa_results-800.webp 800w,/assets/img/blog/fetaqa_results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/fetaqa_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Results of retrieval experiments on the FeTaQA<sup id="fnref:4:5" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup> dataset. The number of row samples included in the serialization is shown on the x-axis, the y-axis shows the retrieval performance measured by recall@3.</p> <h3 id="row-count-counts">Row count counts</h3> <p>The number of rows included in the serialization has a large impact on the retrieval performance. In settings where no metadata is included, embedding more rows generally improves retrieval performance, but with diminishing returns. Most models show little improvement when including more than 10 rows.</p> <p>Looking at the setting with metadata shows stark differences between embedding models. For some long-context embedding models like <em>gte-large-en-v1.5</em>, retrieval performance continues to improve as more rows are included. This suggests that these models are well equipped to handle larger inputs and can leverage the additional information provided by more rows. Less capable embedding models like <em>jina-embeddings-v2-base-en</em> show deteriorating retrieval performance with more rows, suggesting a loss of contextual information in the embeddings.</p> <p>This highlights the importance of carefully selecting the number of rows to include in the serialization, depending on the capabilities of the embedding model being used.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ottqa_recall_vs_row_count_no-context_json-480.webp 480w,/assets/img/blog/ottqa_recall_vs_row_count_no-context_json-800.webp 800w,/assets/img/blog/ottqa_recall_vs_row_count_no-context_json-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/ottqa_recall_vs_row_count_no-context_json.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Experimental results on the OTTQA<sup id="fnref:3:2" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> dataset.</p> <h3 id="not-all-embedding-models-are-created-equal">Not all embedding models are created equal</h3> <p>Our experiments also reveal that different embedding models exhibit varying sensitivities to the serialization parameters. For instance, stronger models like <em>gte-large-en-v1.5</em> perform well when combining a small sample of rows with meaningful metadata, while other models like <em>jina-embeddings-v2-base</em> struggle with this combination.</p> <p>This sensitivity to serialization parameters is highly model-dependent, and there is no single “best” parameter combination that generalizes across all models. Achieving the best performance requires thorough experimentation with each embedding model to identify the optimal serialization strategy.</p> <p>For example, the plots above show how different models respond to the inclusion of metadata and varying row counts. Some models benefit from adding rows alongside metadata, while others show performance degradation, suggesting a loss of contextual information in the embeddings.</p> <h3 id="conclusion">Conclusion</h3> <p>Our experiments demonstrate that the design of table serializations has a significant impact on retrieval performance. Including contextual metadata, carefully selecting the number of rows, and tailoring the serialization strategy to the specific embedding model are all critical factors in achieving optimal results. Additionally, context-length constraints need to be taken into consideration. While we take a first step in exploring table serialization through our experiments, further research in this area is needed.</p> <p>We encourage researchers and practitioners to experiment with different serialization methods using the <a href="https://github.com/daniel-gomm/table-serialization-kitchen">Table Serialization Kitchen</a> package to identify the best approach for their specific use case. By doing so, you can unlock the full potential of LLMs for tasks involving tabular data.</p> <p>Happy experimenting!</p> <hr/> <p>If you find this project useful, please cite it as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{
    gomm2025metadata,
    title={Metadata Matters in Dense Table Retrieval},
    author={Daniel Gomm and Madelon Hulsebos},
    booktitle={ELLIS workshop on Representation Learning and Generative Models for Structured Data},
    year={2025},
    url={https://openreview.net/forum?id=rELWIvq2Qy},
    pdf={https://openreview.net/pdf?id=rELWIvq2Qy}
}
</code></pre></div></div> <p><strong>References:</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:4" role="doc-endnote"> <p>Nan, L., et al. “<a href="https://aclanthology.org/2022.tacl-1.3/">FeTaQA: Free-form Table Question Answering</a>” in <em>Transactions of the Association for Computational Linguistics</em>, vol. 10, pp. 35–49, 2022. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:4:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:4:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:4:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a> <a href="#fnref:4:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a></p> </li> <li id="fn:2" role="doc-endnote"> <p>X. Ji, A. Parameswaran, M. Hulsebos, “<a href="https://target-benchmark.github.io/">TARGET: Benchmarking Table Retrieval for Generative Tasks</a>,” in <em>NeurIPS 2024 Third Table Representation Learning Workshop</em>, 2024. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:3" role="doc-endnote"> <p>W. Chen, et al, “<a href="https://openreview.net/forum?id=MmCRswl1UYl">Open Question Answering over Tables and Text</a>,” in <em>International Conference on Learning Representations</em>, 2021. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> <li id="fn:5" role="doc-endnote"> <p>Yu, T., et al, “<a href="https://aclanthology.org/D18-1425/">Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task</a>,” in <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, 2018, pp. 3911–3921. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:6" role="doc-endnote"> <p>Li, J., et al, “<a href="https://dl.acm.org/doi/abs/10.5555/3666122.3667957">Can LLM already serve as a database interface? a big bench for large-scale database grounded text-to-SQLs</a>,” in <em>Proceedings of the 37th International Conference on Neural Information Processing Systems</em>, 2023. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:6:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="project"/><category term="trl"/><category term="retrieval"/><category term="code"/><category term="guide"/><summary type="html"><![CDATA[This blog post explores table serializations and their impact on downstream tasks using the Table Serialization Kitchen.]]></summary></entry></feed>