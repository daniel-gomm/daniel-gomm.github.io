<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://daniel-gomm.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://daniel-gomm.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-05T15:34:38+00:00</updated><id>https://daniel-gomm.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Daniel Gomm. </subtitle><entry><title type="html">Table Serialization Kitchen - A Recipe for Better LLM Performance on Tabular Data</title><link href="https://daniel-gomm.github.io/blog/2025/Table-Serialization-Kitchen/" rel="alternate" type="text/html" title="Table Serialization Kitchen - A Recipe for Better LLM Performance on Tabular Data"/><published>2025-03-04T00:00:00+00:00</published><updated>2025-03-04T00:00:00+00:00</updated><id>https://daniel-gomm.github.io/blog/2025/Table%20Serialization%20Kitchen</id><content type="html" xml:base="https://daniel-gomm.github.io/blog/2025/Table-Serialization-Kitchen/"><![CDATA[<blockquote> <p>TL;DR: This blog post explores the importance of table serialization on the performance of Large Language Models. We introduce the <a href="https://github.com/daniel-gomm/table-serialization-kitchen">Table Serialization Kitchen</a>, an easy-to-use open-source tool for experimenting with serialization strategies. Initial experiments in table retrieval show large performance differences between different serializations. Metadata can provide highly relevant signal and improve performance significantly. Besides this, we find that there exist no single best serialization strategy but rather that the serialization strategy has to be tailored to the embedding model. Overall, our findings suggest that by optimizing table serialization, you can significantly enhance LLM performance on tabular data.</p> </blockquote> <p>Large Language Models (LLMs) have revolutionized how we interact with data, enabling tasks like question answering, text-to-SQL generation, and more. However, these models typically require text-based inputs, which poses a challenge when working with tabular data. To apply LLMs to tables, we need to convert the structured data into a textual format—a process known as <strong>table serialization</strong>. But how do we serialize tables effectively? And does the way we serialize them impact the performance of downstream tasks?</p> <p>In this blog post, we dive into the world of table serialization for dense retrieval tasks. We’ll share our journey of experimenting with different serialization strategies, highlighting key findings and providing practical insights. Along the way, we’ll introduce the <a href="https://github.com/daniel-gomm/table-serialization-kitchen"><strong>Table Serialization Kitchen</strong></a>, an open-source tool we developed to make experimentation easier. (This project was presented at the <a href="https://sites.google.com/view/rl-and-gm-for-sd">ELLIS workshop on Representation Learning and Generative Models for Structured Data 2025</a>. Find our extended abstract <a href="https://openreview.net/forum?id=rELWIvq2Qy">“Metadata Matters in Dense Table Retrieval” here</a>)</p> <h2 id="the-challenge-of-table-serialization">The Challenge of Table Serialization</h2> <p>Let’s start with an example. Consider the following table from the FeTaQA<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup> dataset:</p> <table> <thead> <tr> <th>Year</th> <th>Title</th> <th>Role</th> </tr> </thead> <tbody> <tr> <td>2012</td> <td>From the Rough</td> <td>Edward</td> </tr> <tr> <td>1997</td> <td>The Borrowers</td> <td>Peagreen Clock</td> </tr> <tr> <td>2013</td> <td>In Secret</td> <td>Camille Raquin</td> </tr> <tr> <td>2004</td> <td>Harry Potter and the Prisoner of Azkaban</td> <td>Draco Malfoy</td> </tr> <tr> <td>2017</td> <td>Feed</td> <td>Matt Grey</td> </tr> </tbody> </table> <p>This table is also associated with metadata:</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"table_page_title"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Tom Felton"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"table_section_title"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Films"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>To use this table with an LLM, we need to convert it into a textual format. One possible serialization might look like this:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Metadata:
table_page_title: Tom Felton
table_section_title: Films

Table:
| Year | Title          | Role           |
|------|----------------|----------------|
| 2012 | From the Rough | Edward         |
| 1997 | The Borrowers  | Peagreen Clock |
| 2013 | In Secret      | Camille Raquin |
</code></pre></div></div> <p>This representation includes both the table’s content and its metadata, formatted in Markdown. But is this the best way to serialize the table? Should we include all the rows, or just a subset? How can select such a subset of rows? What about the metadata—is it helpful, or just noise? And how does the format (e.g., Markdown vs. JSON) affect the results?</p> <p>These questions highlight the complexity of table serialization. There are countless design choices to make, and it’s not immediately clear which ones lead to better performance. To answer these questions, we turned to experimentation.</p> <h2 id="setting-up-experiments-with-table-serialization-kitchen-and-target">Setting up experiments with Table Serialization Kitchen and TARGET</h2> <p>To systematically explore the design space of table serialization, we developed the <a href="https://github.com/daniel-gomm/table-serialization-kitchen"><strong>Table Serialization Kitchen</strong></a>—a Python package that makes it easy to define, test, and compare different serialization strategies. The package integrates with the <a href="https://target-benchmark.github.io/">TARGET benchmark</a><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, allowing us to evaluate serialization methods for table retrieval on real-world datasets like FeTaQA<sup id="fnref:4:1" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup>, OTTQA<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, Spider<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">4</a></sup>, and BIRD<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">5</a></sup>.</p> <p>To follow along this blog post, install <a href="https://github.com/daniel-gomm/table-serialization-kitchen">Table Serialization Kitchen</a> with the TARGET integration:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>tableserializer[target]
</code></pre></div></div> <p>Since there are so many parameters to explore, and the number of experiments scales exponentially with the number of parameters varied, we fix the row sampling strategy to random selection and we do not perform any preprocessing of the data in the tables. Besides that we explore the following parameter configurations:</p> <ul> <li>Recipes (Serialization templates): We evaluate two serialization templates, one that only data from the raw table itself, and one that also leaves room for metadata.</li> <li>Raw table serialization: We compare serializing raw table contents as row-wise JSON and in markdown format.</li> <li>Row sampling: We investigate the impact of varying the number of rows sampled for the serialization in a range of 1-30 sampled rows.</li> </ul> <p>Table serialization kitchen makes creating configurations for all of these settings straight forward:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tableserializer.kitchen</span> <span class="kn">import</span> <span class="n">ExperimentalSerializerKitchen</span>
<span class="kn">from</span> <span class="n">tableserializer.serializer.table</span> <span class="kn">import</span> <span class="n">MarkdownRawTableSerializer</span><span class="p">,</span> <span class="n">JSONRawTableSerializer</span>
<span class="kn">from</span> <span class="n">tableserializer.serializer.metadata</span> <span class="kn">import</span> <span class="n">PairwiseMetadataSerializer</span>
<span class="kn">from</span> <span class="n">tableserializer.table.row_sampler</span> <span class="kn">import</span> <span class="n">RandomRowSampler</span>
<span class="kn">from</span> <span class="n">tableserializer</span> <span class="kn">import</span> <span class="n">SerializationRecipe</span>


<span class="c1"># Define all the different configurations of components to use in the experiments
</span>
<span class="c1"># Recipes define the general structure of the output
</span><span class="n">recipes</span> <span class="o">=</span> <span class="p">[</span><span class="nc">SerializationRecipe</span><span class="p">(</span><span class="sh">"</span><span class="s">Metadata:</span><span class="se">\n</span><span class="s">{META}</span><span class="se">\n\n</span><span class="s">Table:</span><span class="se">\n</span><span class="s">{TABLE}</span><span class="sh">"</span><span class="p">),</span>
           <span class="nc">SerializationRecipe</span><span class="p">(</span><span class="sh">"</span><span class="s">Table:</span><span class="se">\n</span><span class="s">{TABLE}</span><span class="sh">"</span><span class="p">)]</span>

<span class="c1"># Metadata serializers define how metadata is parsed and serialized
</span><span class="n">metadata_serializers</span> <span class="o">=</span> <span class="p">[</span><span class="nc">PairwiseMetadataSerializer</span><span class="p">()]</span>

<span class="c1"># Raw table serializers define how raw tables are serialized
</span><span class="n">raw_table_serializers</span> <span class="o">=</span> <span class="p">[</span><span class="nc">MarkdownRawTableSerializer</span><span class="p">(),</span> <span class="nc">JSONRawTableSerializer</span><span class="p">()]</span>

<span class="c1"># Row samplers define how and how many rows are sampled for the raw table serialization
# Here we just use random row samplers, sampling 1-30 rows
</span><span class="n">row_samplers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">+</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">5</span><span class="p">)):</span>
    <span class="n">row_samplers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">RandomRowSampler</span><span class="p">(</span><span class="n">rows_to_sample</span><span class="o">=</span><span class="n">i</span><span class="p">))</span>

<span class="c1"># The experimental serializer kitchen is used to manage table serializers for experimentation
</span><span class="n">kitchen</span> <span class="o">=</span> <span class="nc">ExperimentalSerializerKitchen</span><span class="p">()</span>

<span class="c1"># Create an array of serializers with different configurations (grid-search-style combination of all parameters)
</span><span class="n">serializers</span> <span class="o">=</span> <span class="n">kitchen</span><span class="p">.</span><span class="nf">create_serializers</span><span class="p">(</span><span class="n">recipes</span><span class="o">=</span><span class="n">recipes</span><span class="p">,</span>
                                         <span class="n">schema_serializers</span><span class="o">=</span><span class="p">[],</span>
                                         <span class="n">metadata_serializers</span><span class="o">=</span><span class="n">metadata_serializers</span><span class="p">,</span>
                                         <span class="n">table_serializers</span><span class="o">=</span><span class="n">raw_table_serializers</span><span class="p">,</span>
                                         <span class="n">row_samplers</span><span class="o">=</span><span class="n">row_samplers</span><span class="p">,</span>
                                         <span class="n">table_preprocessor_constellations</span><span class="o">=</span><span class="p">[[]])</span>

<span class="c1"># Save the serializers in a folder structure
</span><span class="n">kitchen</span><span class="p">.</span><span class="nf">save_serializer_experiment_configurations</span><span class="p">(</span><span class="n">serializers</span><span class="p">,</span> <span class="sh">"</span><span class="s">./retrieval_experiments</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This creates a folder structure that contains the configurations for all the different combinations of parameters. We can now run these experiments on a dataset (FeTaQA<sup id="fnref:4:2" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup> in this case) using the TARGET integration into table serializer kitchen:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tableserializer.integrations.target</span> <span class="kn">import</span> <span class="n">TARGETOpenAIExperimentExecutor</span>
<span class="kn">from</span> <span class="n">tableserializer.kitchen</span> <span class="kn">import</span> <span class="n">ExperimentalSerializerKitchen</span>

<span class="c1"># Create experiment executor that can run a TARGET experiment
</span><span class="n">experiment_executor</span> <span class="o">=</span> <span class="nc">TARGETOpenAIExperimentExecutor</span><span class="p">(</span><span class="sh">"</span><span class="s">INSERT_API_KEY_HERE</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fetaqa</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">,</span>
                                                     <span class="n">embedding_cache_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">./retrieval_experiments/.cache</span><span class="sh">"</span><span class="p">)</span>

<span class="n">kitchen</span> <span class="o">=</span> <span class="nc">ExperimentalSerializerKitchen</span><span class="p">()</span>

<span class="c1"># Run all experiments in the directory
</span><span class="n">kitchen</span><span class="p">.</span><span class="nf">run_experiments_with_serializers</span><span class="p">(</span><span class="n">base_folder</span><span class="o">=</span><span class="sh">"</span><span class="s">./retrieval_experiments</span><span class="sh">"</span><span class="p">,</span>
                                         <span class="n">experiment_callback</span><span class="o">=</span><span class="n">experiment_executor</span><span class="p">.</span><span class="n">run_experiment</span><span class="p">)</span>
</code></pre></div></div> <h2 id="how-to-serialize-tables-for-table-retrieval">How to serialize tables for table retrieval?</h2> <p>We use TARGET<sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> to benchmark table retrieval with the different serializers. TARGET provide a unified interface to run table retrieval benchmarks on datasets from different tasks like TabularQA (e.g., FeTaQA<sup id="fnref:4:3" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup>, OTTQA<sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>) or Text-to-SQL (Spider<sup id="fnref:5:1" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">4</a></sup>, BIRD<sup id="fnref:6:1" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">5</a></sup>). These experiments show large deviations in retrieval performance depending on how the serializations are constructed.</p> <h3 id="metadata-matters">Metadata Matters</h3> <p>One of the most striking results from our experiments is the significant impact of including contextual metadata in the table serialization. In the FeTaQA<sup id="fnref:4:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup> dataset, tables are associated with page and section titles as metadata. When this metadata is included in the serialized representation, retrieval performance improves substantially.</p> <p>For example, as shown in the plots below, including metadata increases the average recall@3 by <strong>0.42</strong>. This demonstrates that contextualizing tables with relevant metadata is crucial for effective retrieval. The metadata provides additional context that helps the embedding models better understand the relevance of the table to a given query.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/fetaqa_results-480.webp 480w,/assets/img/blog/fetaqa_results-800.webp 800w,/assets/img/blog/fetaqa_results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/fetaqa_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Results of retrieval experiments on the FeTaQA<sup id="fnref:4:5" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup> dataset. The number of row samples included in the serialization is shown on the x-axis, the y-axis shows the retrieval performance measured by recall@3.</p> <h3 id="row-count-counts">Row count counts</h3> <p>The number of rows included in the serialization has a large impact on the retrieval performance. In settings where no metadata is included, embedding more rows generally improves retrieval performance, but with diminishing returns. Most models show little improvement when including more than 10 rows.</p> <p>Looking at the setting with metadata shows stark differences between embedding models. For some long-context embedding models like <em>gte-large-en-v1.5</em>, retrieval performance continues to improve as more rows are included. This suggests that these models are well equipped to handle larger inputs and can leverage the additional information provided by more rows. Less capable embedding models like <em>jina-embeddings-v2-base-en</em> show deteriorating retrieval performance with more rows, suggesting a loss of contextual information in the embeddings.</p> <p>This highlights the importance of carefully selecting the number of rows to include in the serialization, depending on the capabilities of the embedding model being used.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ottqa_recall_vs_row_count_no-context_json-480.webp 480w,/assets/img/blog/ottqa_recall_vs_row_count_no-context_json-800.webp 800w,/assets/img/blog/ottqa_recall_vs_row_count_no-context_json-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/ottqa_recall_vs_row_count_no-context_json.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Experimental results on the OTTQA<sup id="fnref:3:2" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> dataset.</p> <h3 id="not-all-embedding-models-are-created-equal">Not all embedding models are created equal</h3> <p>Our experiments also reveal that different embedding models exhibit varying sensitivities to the serialization parameters. For instance, stronger models like <em>gte-large-en-v1.5</em> perform well when combining a small sample of rows with meaningful metadata, while other models like <em>jina-embeddings-v2-base</em> struggle with this combination.</p> <p>This sensitivity to serialization parameters is highly model-dependent, and there is no single “best” parameter combination that generalizes across all models. Achieving the best performance requires thorough experimentation with each embedding model to identify the optimal serialization strategy.</p> <p>For example, the plots above show how different models respond to the inclusion of metadata and varying row counts. Some models benefit from adding rows alongside metadata, while others show performance degradation, suggesting a loss of contextual information in the embeddings.</p> <h3 id="conclusion">Conclusion</h3> <p>Our experiments demonstrate that the design of table serializations has a significant impact on retrieval performance. Including contextual metadata, carefully selecting the number of rows, and tailoring the serialization strategy to the specific embedding model are all critical factors in achieving optimal results. Additionally, context-length constraints need to be taken into consideration. While we take a first step in exploring table serialization through our experiments, further research in this area is needed.</p> <p>We encourage researchers and practitioners to experiment with different serialization methods using the <a href="https://github.com/daniel-gomm/table-serialization-kitchen">Table Serialization Kitchen</a> package to identify the best approach for their specific use case. By doing so, you can unlock the full potential of LLMs for tasks involving tabular data.</p> <p>Happy experimenting!</p> <hr/> <p>If you find this project useful, please cite it as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{
    gomm2025metadata,
    title={Metadata Matters in Dense Table Retrieval},
    author={Daniel Gomm and Madelon Hulsebos},
    booktitle={ELLIS workshop on Representation Learning and Generative Models for Structured Data},
    year={2025},
    url={https://openreview.net/forum?id=rELWIvq2Qy},
    pdf={https://openreview.net/pdf?id=rELWIvq2Qy}
}
</code></pre></div></div> <p><strong>References:</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:4" role="doc-endnote"> <p>Nan, L., et al. “<a href="https://aclanthology.org/2022.tacl-1.3/">FeTaQA: Free-form Table Question Answering</a>” in <em>Transactions of the Association for Computational Linguistics</em>, vol. 10, pp. 35–49, 2022. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:4:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:4:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:4:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a> <a href="#fnref:4:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a></p> </li> <li id="fn:2" role="doc-endnote"> <p>X. Ji, A. Parameswaran, M. Hulsebos, “<a href="https://target-benchmark.github.io/">TARGET: Benchmarking Table Retrieval for Generative Tasks</a>,” in <em>NeurIPS 2024 Third Table Representation Learning Workshop</em>, 2024. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:3" role="doc-endnote"> <p>W. Chen, et al, “<a href="https://openreview.net/forum?id=MmCRswl1UYl">Open Question Answering over Tables and Text</a>,” in <em>International Conference on Learning Representations</em>, 2021. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> <li id="fn:5" role="doc-endnote"> <p>Yu, T., et al, “<a href="https://aclanthology.org/D18-1425/">Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task</a>,” in <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, 2018, pp. 3911–3921. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:6" role="doc-endnote"> <p>Li, J., et al, “<a href="https://dl.acm.org/doi/abs/10.5555/3666122.3667957">Can LLM already serve as a database interface? a big bench for large-scale database grounded text-to-SQLs</a>,” in <em>Proceedings of the 37th International Conference on Neural Information Processing Systems</em>, 2023. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:6:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="project"/><category term="trl"/><category term="retrieval"/><category term="code"/><category term="guide"/><summary type="html"><![CDATA[This blog post explores table serializations and their impact on downstream tasks using the Table Serialization Kitchen.]]></summary></entry></feed>